{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28304da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "def readdata_MT5(market,tf,barre,path):\n",
    "    from datetime import datetime\n",
    "    import MetaTrader5 as mt5\n",
    "    import pandas as pd\n",
    "    \n",
    "\n",
    "    while True:\n",
    "\n",
    "        if not mt5.initialize(path):\n",
    "            print(\"initialize() failed, error code =\",mt5.last_error())\n",
    "            quit()\n",
    "        mk=str(market)\n",
    "        TF=\"mt5.TIMEFRAME_\"+str(tf)\n",
    "        bar=barre\n",
    "\n",
    "        rates = mt5.copy_rates_from_pos(mk, eval(TF), 0, bar) \n",
    "    \n",
    "        try:\n",
    "            rates_frame = pd.DataFrame(rates)\n",
    "            rates_frame['time']=pd.to_datetime(rates_frame['time'], unit='s')\n",
    "            rates_frame=rates_frame[[\"time\",\"open\",\"high\",\"low\",\"close\",\"real_volume\"]]\n",
    "            rates_frame.columns=([\"time\",\"open\",\"high\",\"low\",\"close\",\"volume\"])\n",
    "            rates_frame=rates_frame.set_index([\"time\"])\n",
    "            rates_frame[\"dayofweek\"] = rates_frame.index.dayofweek\n",
    "            rates_frame[\"day\"] = rates_frame.index.day\n",
    "            rates_frame[\"month\"] = rates_frame.index.month\n",
    "            rates_frame[\"year\"] = rates_frame.index.year\n",
    "            rates_frame[\"dayofyear\"] = rates_frame.index.dayofyear\n",
    "            rates_frame[\"quarter\"] = rates_frame.index.quarter\n",
    "            rates_frame[\"hour\"] = rates_frame.index.hour\n",
    "            rates_frame[\"minute\"] = rates_frame.index.minute\n",
    "            rates_frame[\"daily_open\"] = daily_open(rates_frame,1)\n",
    "            rates_frame[\"daily_high\"] = daily_high(rates_frame,1)\n",
    "            rates_frame[\"daily_low\"] = daily_low(rates_frame,1)\n",
    "            rates_frame[\"daily_close\"] = daily_close(rates_frame,1)\n",
    "            rates_frame[\"AVGPRICE\"]=ta.AVGPRICE(rates_frame.open, rates_frame.high, rates_frame.low, rates_frame.close)\n",
    "            rates_frame[\"MEDPRICE\"]=ta.MEDPRICE(rates_frame.high, rates_frame.low)\n",
    "            rates_frame[\"TYPPRICE\"]=ta.TYPPRICE(rates_frame.high, rates_frame.low, rates_frame.close)\n",
    "            rates_frame[\"WCLPRICE\"]=ta.WCLPRICE(rates_frame.high, rates_frame.low, rates_frame.close)\n",
    "            \n",
    "            if rates_frame is not None:\n",
    "                return rates_frame\n",
    "                break\n",
    "        except KeyError:\n",
    "            continue \n",
    "            \n",
    "def tick_info(simbolo):\n",
    "    import MetaTrader5 as mt5\n",
    "    tick_info = mt5.symbol_info(simbolo)\n",
    "    tick_info = float(tick_info.trade_tick_size)\n",
    "    return tick_info\n",
    "\n",
    "def Big_Point_Values(simbolo,path):\n",
    "    import MetaTrader5 as mt5\n",
    "    if not mt5.initialize(path):\n",
    "        print(\"initialize() failed, error code =\",mt5.last_error())\n",
    "        quit()    \n",
    "    \n",
    "    symbol_info_dict = mt5.symbol_info(simbolo)._asdict()\n",
    "    #display(symbol_info_dict)\n",
    "    bpv=symbol_info_dict['trade_tick_value']  /symbol_info_dict['point']\n",
    "    return bpv    \n",
    "            \n",
    "def profitto(operations):\n",
    "    return operations.sum()\n",
    "\n",
    "def EV(a):\n",
    "    PW=float(a.PctWin)/100\n",
    "    PL=float(100-float(a.PctWin))/100\n",
    "    AW=int(a.gross_profit)\n",
    "    AL=abs(int(a.gross_loss))\n",
    "    EV=(PW*AW)-(PL*AL)\n",
    "    return EV\n",
    "\n",
    "def performance_report_gen(tradelist):\n",
    "    res=pd.DataFrame()\n",
    "    d = {\"Profit\": profitto(tradelist),\n",
    "         \"MaxDD\" : max_draw_down(tradelist),\n",
    "         \"Operations\":operation_number(tradelist),\n",
    "         \"AverageTrade\": avg_trade(tradelist),\n",
    "         \"ProfitFactor\": profit_factor(tradelist),\n",
    "         \"PctWin\": percent_win(tradelist),\n",
    "         \"KestnerRatio\": kestner_ratio(tradelist),\n",
    "         \"STD_Daily\" : tradelist.resample(\"D\").sum().std(),\n",
    "         \"Skew\" : skew(tradelist.values),\n",
    "         \"Reward_Risk_Ratio\": reward_risk_ratio(tradelist)}\n",
    "    \n",
    "    res=pd.DataFrame(data=d,index=[0])\n",
    "    return res \n",
    "\n",
    "def test_time_frame(tf):\n",
    "    tf=tf.lower()\n",
    "    ls_tf=[\"1min\",\"5min\",\"15min\",\"30min\",\"60min\",\"120min\",\"240min\",\"480min\",\"1380min\",\"1440min\"]\n",
    "    test_ok=0\n",
    "    for e in ls_tf:\n",
    "        if e == tf:\n",
    "            tf_ok=e\n",
    "            test_ok=1\n",
    "            break\n",
    "    if test_ok == 0:\n",
    "            print(\"ERRORE - Solo time frame a: \",ls_tf,\" in ingresso\")\n",
    "    else:\n",
    "        tf_ok=tf_ok.split(\"min\")[0]\n",
    "        return int(tf_ok)\n",
    "    \n",
    "def resample_standard_session(original_tf,resample_tf,dataset):\n",
    "    \n",
    "    original_tf=test_time_frame(original_tf)\n",
    "    resample_tf=test_time_frame(resample_tf)\n",
    "    df_0=dataset.copy()\n",
    "    \n",
    "    df_0.index=df_0.index - pd.DateOffset(minutes=original_tf)\n",
    "    df_resample=df_0.resample(str(resample_tf)+'Min').agg({'open' : 'first', 'high' : 'max', 'low' : 'min', 'close' : 'last','volume':'sum'})\n",
    "    \n",
    "    if resample_tf == \"1440\" :\n",
    "        df_resample = df_resample\n",
    "    else:\n",
    "        df_resample.index=df_resample.index + pd.DateOffset(minutes=resample_tf)\n",
    "        \n",
    "    df_resample=df_resample.dropna()\n",
    "    return df_resample\n",
    "\n",
    "def load_data_intraday_fast(filename):\n",
    "    data=pd.read_csv(filename,engine=\"pyarrow\")\n",
    "    data.set_index([\"date_time\"],inplace=True)\n",
    "    data.index=pd.to_datetime(data.index)\n",
    "    return data\n",
    "\n",
    "def resample_custom_session(original_tf,resample_tf,dataset,start_time,end_time):\n",
    "    \n",
    "    original_tf=test_time_frame(original_tf)\n",
    "    resample_tf=test_time_frame(resample_tf)\n",
    "    df_0=dataset.copy()\n",
    "    \n",
    "    df_0.index=df_0.index - pd.DateOffset(minutes=original_tf)\n",
    "    \n",
    "    test_start_time=0\n",
    "    if len(start_time) == 4:\n",
    "        st_h=start_time[:2]\n",
    "        st_m=start_time[2:]\n",
    "        test_start_time=1\n",
    "        \n",
    "    test_end_time=0\n",
    "    if len(end_time) == 4:\n",
    "        en_h=end_time[:2]\n",
    "        en_m=end_time[2:]\n",
    "        test_end_time=1\n",
    "\n",
    "    if (test_start_time==0)|(test_end_time==0):\n",
    "        print(\"ERRORE - controlla gli orari di start & end\")\n",
    "    else:\n",
    "\n",
    "        dati=df_0.loc[(df_0.index.time>=datetime.time(int(st_h), int(st_m)))&(df_0.index.time<=datetime.time(int(en_h), int(en_m)))]\n",
    "        dati= dati.resample(str(resample_tf)+'Min',origin=st_h+\":\"+st_m+':00').agg({'open': 'first','high': 'max','low':'min','close':'last','volume':'sum'}).dropna()\n",
    "        dati.index=dati.index + pd.DateOffset(minutes=resample_tf)\n",
    "        \n",
    "        test_h=sorted(list(set(dati.index.time)))\n",
    "        \n",
    "        time1 = datetime.time(int(en_h),int(en_m))\n",
    "        timedelta = datetime.timedelta(minutes=original_tf)\n",
    "        tmp_datetime = datetime.datetime.combine(datetime.date(1, 1, 1), time1)\n",
    "        time2 = (tmp_datetime - timedelta).time()\n",
    "        \n",
    "        dati1=df_0.loc[(df_0.index.time>=test_h[-2])&(df_0.index.time<=time2)]\n",
    "        dati1= dati1.resample(\"D\").agg({'open': 'first','high': 'max','low':'min','close':'last','volume':'sum'}).dropna()\n",
    "        dati1.index=dati1.index + pd.DateOffset(hours=int(en_h),minutes=int(en_m))\n",
    "\n",
    "        custom_df=dati.loc[dati.index.time!=test_h[-1]].dropna()\n",
    "        custom_df=pd.concat([custom_df,dati1])\n",
    "        custom_df=custom_df.sort_index(ascending=True)\n",
    "        return custom_df\n",
    "    \n",
    "def carica_storico(file_name,uct_offset,type_session,resample_tf,IS,OOS,custom_session_start,custom_session_stop,noise,pct_noise):\n",
    "    import os\n",
    "    import talib as ta\n",
    "    \n",
    "    os.chdir(dir_history)\n",
    "    data = load_data_intraday_fast(file_name)  \n",
    "    print(\"Caricato storico\")\n",
    "    data = data.sort_index(ascending=True)\n",
    "    data.index = data.index.tz_localize('Etc/Zulu')\n",
    "    data.index = data.index.tz_convert(uct_offset )\n",
    "    data.index = data.index.tz_localize(None)\n",
    "    data = data[data!=0]\n",
    "    #data = data.fillna(method=\"ffill\")\n",
    "    data = data.iloc[1:-1]\n",
    "    print(\"Resample dei dati\")\n",
    "    if type_session == 1:\n",
    "        dataset=resample_custom_session(\"5min\",resample_tf ,data,custom_session_start,custom_session_stop) \n",
    "    if type_session == 0:    \n",
    "        dataset=resample_standard_session(\"5min\",resample_tf,data)\n",
    "        \n",
    "    if noise == 1:\n",
    "        print(\"AGGIUNGO RUMORE\")\n",
    "        dataset=add_noise(dataset.open, dataset.high, dataset.low, dataset.close,dataset.volume, pct_noise)\n",
    "\n",
    "    dataset = dataset.loc[IS:OOS]\n",
    "    dataset[\"dayofweek\"] = dataset.index.dayofweek\n",
    "    dataset[\"day\"] = dataset.index.day\n",
    "    dataset[\"month\"] = dataset.index.month\n",
    "    dataset[\"year\"] = dataset.index.year\n",
    "    dataset[\"dayofyear\"] = dataset.index.dayofyear\n",
    "    dataset[\"week\"] = dataset.index.week\n",
    "    dataset[\"hour\"] = dataset.index.hour\n",
    "    dataset[\"minute\"] = dataset.index.minute\n",
    "    dataset[\"daily_open\"] = daily_open(dataset,1)\n",
    "    dataset[\"daily_high\"] = daily_high(dataset,1)\n",
    "    dataset[\"daily_low\"] = daily_low(dataset,1)\n",
    "    dataset[\"daily_close\"] = daily_close(dataset,1)\n",
    "    dataset[\"AVGPRICE\"]=ta.AVGPRICE(dataset.open, dataset.high, dataset.low, dataset.close)\n",
    "    dataset[\"MEDPRICE\"]=ta.MEDPRICE(dataset.high, dataset.low)\n",
    "    dataset[\"TYPPRICE\"]=ta.TYPPRICE(dataset.high, dataset.low, dataset.close)\n",
    "    dataset[\"WCLPRICE\"]=ta.WCLPRICE(dataset.high, dataset.low, dataset.close)\n",
    "    history=dataset.copy()\n",
    "    \n",
    "    print(\"Carico indicatori\")\n",
    "\n",
    "    \n",
    "    import legitindicators\n",
    "\n",
    "    #data_filter=dataset.iloc[:].copy()\n",
    "    #for e in filter.columns:\n",
    "        #data_filter[e]= legitindicators.super_smoother(filter[e], 10)\n",
    "    \n",
    "    #import talib as ta\n",
    "    #ATR = ta.ATR(dataset.high,dataset.low,dataset.close,5)\n",
    "    #ATR = ATR.fillna(0)\n",
    "    #dataATR=dataset.copy()\n",
    "    #for e in dataATR.columns:\n",
    "        #dataATR[e]=dataATR[e]/ATR\n",
    "        \n",
    "    applica_indicatori=apply_indicator(dataset)\n",
    "    dataset=applica_indicatori[0]\n",
    "    dataset_ind = dataset.iloc[:,21:].copy()\n",
    "\n",
    "\n",
    "    rules=dataset.iloc[:,21:].T.values\n",
    "    rule_formulas=np.arange(len(rules))\n",
    "    \n",
    "    history.tail(10)\n",
    "    \n",
    "    history.close.plot(figsize=(20,10),title=SIMBOLO+\"_\"+RESAMPLE_TF)\n",
    "    print(\"Fatto\")\n",
    "    return dataset , dataset_ind , history , rules, rule_formulas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d1cdbe-8945-444d-ad52-f923cef7296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def randrange_float(start, stop, step):\n",
    "    return random.randint(0, int((stop - start) / step)) * step + start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aed199-01d1-4e3c-b559-39d6a585d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def environment(simbolo,type_session,resample_tf,start_date,end_date):\n",
    "               \n",
    "    file_name = check_history_name(simbolo,dir_history)\n",
    "\n",
    "    IS = start_date\n",
    "    OOS = end_date\n",
    "\n",
    "    bigpointvalue=valori[valori.Physical==simbolo].BigPointValue.values[0]\n",
    "    tick=valori[valori.Physical==simbolo].Tick.values[0]\n",
    "    utc_offset = valori[valori.Physical==simbolo].Exchange_Time.values[0]\n",
    "    custom_session_start = \"0\"+str(valori[valori.Physical==simbolo][\"Custom Open\"].values[0])\n",
    "    custom_session_stop = str(valori[valori.Physical==simbolo][\"Custom Close\"].values[0])\n",
    "    costi=valori[valori.Physical==simbolo].Cost.values[0]\n",
    "\n",
    "    dataset , dataset_ind , history , rules, rule_formulas = carica_storico(file_name,utc_offset,type_session,resample_tf,IS,OOS,custom_session_start,custom_session_stop,0,0)\n",
    "    \n",
    "    return dataset , dataset_ind , history , rules, rule_formulas , bigpointvalue , tick , utc_offset , custom_session_start , custom_session_stop , costi \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91772bf5-51cd-4d99-87eb-eb4f097d05bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OPEN(dataset):\n",
    "    return dataset[\"open\"]\n",
    "def HIGH(dataset):\n",
    "    return dataset[\"high\"]\n",
    "def LOW(dataset):\n",
    "    return dataset[\"low\"]\n",
    "def CLOSE(dataset):\n",
    "    return dataset[\"close\"]\n",
    "\n",
    "def High_Close(serie):\n",
    "    return (serie.iloc[:,1] - serie.iloc[:,3])\n",
    "def High_Open(serie):\n",
    "    return (serie.iloc[:,1] - serie.iloc[:,0])\n",
    "def Low_Close(serie):\n",
    "    return (serie.iloc[:,2] - serie.iloc[:,3])\n",
    "def Low_Open(serie):\n",
    "    return (serie.iloc[:,2] - serie.iloc[:,0])\n",
    "\n",
    "def avg_true_range(dataframe, period):\n",
    "    dataframe[\"M1\"] = dataframe.high - dataframe.low\n",
    "    dataframe[\"M2\"] = abs(dataframe.high - dataframe.low.shift(1)).fillna(0)\n",
    "    dataframe[\"M3\"] = abs(dataframe.low - dataframe.close.shift(1)).fillna(0)\n",
    "    dataframe[\"Max\"] = dataframe[[\"M1\", \"M2\", \"M3\"]].max(axis = 1)\n",
    "    dataframe[\"MeanMax\"] = dataframe[\"Max\"].rolling(period).mean()\n",
    "    return dataframe.MeanMax.fillna(0)\n",
    "\n",
    "def RSI(series, period):\n",
    "    \"\"\"\n",
    "    Function to calculate the Relative Strength Index of a close serie\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(series, index = series.index)\n",
    "    df[\"chg\"] = series.diff(1)\n",
    "    df[\"gain\"] = np.where(df.chg > 0, 1, 0)\n",
    "    df[\"loss\"] = np.where(df.chg <= 0, 1, 0)\n",
    "    df[\"avg_gain\"] = df.gain.rolling(period).sum() / period * 100\n",
    "    df[\"avg_loss\"] = df.loss.rolling(period).sum() / period * 100\n",
    "    rs = abs(df[\"avg_gain\"] / df[\"avg_loss\"])\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def MFI(df, period):\n",
    "    \"\"\"\n",
    "    Function to calculate the Money Flow Index of a price serie (OHLCV)\n",
    "    \"\"\"\n",
    "    df[\"typical_price\"] = (df.iloc[:,1] + df.iloc[:,2] + df.iloc[:,3]) / 3\n",
    "    df[\"raw_money_flow\"] = df.typical_price * df.iloc[:,4]\n",
    "    df[\"chg\"] = df.raw_money_flow.diff(1)\n",
    "    df[\"pos_money_flow\"] = np.where(df.chg > 0,1,0)\n",
    "    df[\"neg_money_flow\"] = np.where(df.chg <= 0,1,0)\n",
    "    df[\"avg_gain\"] = df.pos_money_flow.rolling(period).sum() / period * 100\n",
    "    df[\"avg_loss\"] = df.neg_money_flow.rolling(period).sum() / period * 100\n",
    "    mfr = abs(df[\"avg_gain\"] / df[\"avg_loss\"])\n",
    "    mfi = 100 - (100 / (1 + mfr))\n",
    "    return mfi\n",
    "\n",
    "def BollingerBand(serie,period,multiplier,upper_lower):\n",
    "    if upper_lower == 0:\n",
    "        return serie.rolling(period).mean() + multiplier * serie.rolling(period).std()\n",
    "    if upper_lower == 1:\n",
    "        return serie.rolling(period).mean() - multiplier - serie.rolling(period).std()\n",
    "\n",
    "def ROC(serie,period):\n",
    "    return ((serie / serie.shift(period)) - 1) * 100\n",
    "\n",
    "def Momentum(serie,period):\n",
    "    return (serie - serie.shift(period))\n",
    "\n",
    "def SMA(serie,period):\n",
    "    return serie.rolling(period).mean()\n",
    "\n",
    "def Range(serie):\n",
    "    \"\"\"\n",
    "    Function to calculate the Range of a price serie (OHLCV)\n",
    "    \"\"\"\n",
    "    return (serie.iloc[:,1] - serie.iloc[:,2])\n",
    "\n",
    "def Body(serie):\n",
    "    \"\"\"\n",
    "    Function to calculate the Body of a price serie (OHLCV)\n",
    "    \"\"\"\n",
    "    return (serie.iloc[:,3] - serie.iloc[:,0])\n",
    "\n",
    "def AvgPrice(serie):\n",
    "    \"\"\"\n",
    "    Function to calculate the AvgPrice of a price serie (OHLCV)\n",
    "    \"\"\"\n",
    "    return (serie.iloc[:,0] + serie.iloc[:,1] + serie.iloc[:,2] + serie.iloc[:,3]) / 4\n",
    "\n",
    "def MedPrice(serie):\n",
    "    \"\"\"\n",
    "    Function to calculate the MedPrice of a price serie (OHLCV)\n",
    "    \"\"\"\n",
    "    return (serie.iloc[:,1] + serie.iloc[:,2]) / 2\n",
    "\n",
    "def MedBodyPrice(serie):\n",
    "    \"\"\"\n",
    "    Function to calculate the MedBodyPrice of a price serie (OHLCV)\n",
    "    \"\"\"\n",
    "    return (serie.iloc[:,0] + serie.iloc[:,3]) / 2\n",
    "\n",
    "def Blastoff(serie):\n",
    "    return abs(serie.iloc[:,0] - serie.iloc[:,3]) / (serie.iloc[:,1] - serie.iloc[:,2])\n",
    "\n",
    "def OpenToLowLog(dataset):\n",
    "    data = dataset.copy()\n",
    "    return pd.Series(np.log(data.open) - np.log(data.low))\n",
    "\n",
    "def OpenToAvgPriceLog(dataset):\n",
    "    data = dataset.copy()\n",
    "    data[\"avgprice\"] = data.iloc[:,:4].mean(axis = 1)\n",
    "    return pd.Series(np.log(data.open) - np.log(data.avgprice))\n",
    "\n",
    "def OpenToMedPriceLog(dataset):\n",
    "    data = dataset.copy()\n",
    "    data[\"medprice\"] = (data.high + data.low) / 2\n",
    "    return pd.Series(np.log(data.open) - np.log(data.medprice))\n",
    "\n",
    "def OpenToOpenLog(dataset):\n",
    "    data = dataset.copy()\n",
    "    return pd.Series(np.log(data.open) - np.log(data.open.shift(1)))\n",
    "\n",
    "def OpenToMedBodyPriceLog(dataset):\n",
    "    data = dataset.copy()\n",
    "    data[\"medbodyprice\"] = (data.open + data.close) / 2\n",
    "    return pd.Series(np.log(data.open) - np.log(data.medbodyprice))\n",
    "\n",
    "def CloseToMedPriceLog(dataset):\n",
    "    data = dataset.copy()\n",
    "    data[\"medprice\"] = (data.high + data.low) / 2\n",
    "    return pd.Series(np.log(data.close) - np.log(data.medprice))\n",
    "\n",
    "def CloseToCloseLog(dataset):\n",
    "    data = dataset.copy()\n",
    "    return pd.Series(np.log(data.close) - np.log(data.close.shift(1)))\n",
    "\n",
    "def CloseToLowLog(dataset):\n",
    "    data = dataset.copy()\n",
    "    return pd.Series(np.log(data.close) - np.log(data.low))\n",
    "\n",
    "def CloseToAvgPriceLog(dataset):\n",
    "    data = dataset.copy()\n",
    "    data[\"avgprice\"] = data.iloc[:,:4].mean(axis = 1)\n",
    "    return pd.Series(np.log(data.close) - np.log(data.avgprice))\n",
    "\n",
    "def CloseToMedBodyPriceLog(dataset):\n",
    "    data = dataset.copy()\n",
    "    data[\"medbodyprice\"] = (data.open + data.close) / 2\n",
    "    return pd.Series(np.log(data.close) - np.log(data.medbodyprice))\n",
    "\n",
    "def HighToCloseLog(dataset):\n",
    "    data = dataset.copy()\n",
    "    return pd.Series(np.log(data.high) - np.log(data.close))\n",
    "\n",
    "def HighToOpenLog(dataset):\n",
    "    data = dataset.copy()\n",
    "    return pd.Series(np.log(data.high) - np.log(data.open))\n",
    "\n",
    "def HighToMedBodyPriceLog(dataset):\n",
    "    data = dataset.copy()\n",
    "    data[\"medbodyprice\"] = (data.open + data.close) / 2\n",
    "    return pd.Series(np.log(data.high) - np.log(data.medbodyprice))\n",
    "\n",
    "def HighToMedPriceLog(dataset):\n",
    "    data = dataset.copy()\n",
    "    data[\"medprice\"] = (data.high + data.low) / 2\n",
    "    return pd.Series(np.log(data.high) - np.log(data.medprice))\n",
    "\n",
    "def HighToAvgPriceLog(dataset):\n",
    "    data = dataset.copy()\n",
    "    data[\"avgprice\"] = data.iloc[:,:4].mean(axis = 1)\n",
    "    return pd.Series(np.log(data.high) - np.log(data.avgprice))\n",
    "\n",
    "def HighToHighLog(dataset):\n",
    "    data = dataset.copy()\n",
    "    return pd.Series(np.log(data.high) - np.log(data.high.shift(1)))\n",
    "\n",
    "def LowToLowLog(dataset):\n",
    "    data = dataset.copy()\n",
    "    return pd.Series(np.log(data.low) - np.log(data.low.shift(1)))\n",
    "\n",
    "def AvgPriceToMedPriceLog(dataset):\n",
    "    data = dataset.copy()\n",
    "    data[\"avgprice\"] = data.iloc[:,:4].mean(axis = 1)\n",
    "    data[\"medprice\"] = (data.high + data.low) / 2\n",
    "    return pd.Series(np.log(data.avgprice) - np.log(data.medprice))\n",
    "\n",
    "def AvgPriceToLowLog(dataset):\n",
    "    data = dataset.copy()\n",
    "    data[\"avgprice\"] = data.iloc[:,:4].mean(axis = 1)\n",
    "    return pd.Series(np.log(data.avgprice) - np.log(data.low))\n",
    "\n",
    "def AvgPriceToAvgPriceLog(dataset):\n",
    "    data = dataset.copy()\n",
    "    data[\"avgprice\"] = data.iloc[:,:4].mean(axis = 1)\n",
    "    return pd.Series(np.log(data.avgprice) - np.log(data.avgprice.shift(1)))\n",
    "\n",
    "def AvgPriceToMedBodyPriceLog(dataset):\n",
    "    data = dataset.copy()\n",
    "    data[\"avgprice\"] = data.iloc[:,:4].mean(axis = 1)\n",
    "    data[\"medbodyprice\"] = (data.open + data.close) / 2\n",
    "    return pd.Series(np.log(data.avgprice) - np.log(data.medbodyprice))\n",
    "\n",
    "def MedPriceToLowLog(dataset):\n",
    "    data = dataset.copy()\n",
    "    data[\"medprice\"] = (data.high + data.low) / 2\n",
    "    return pd.Series(np.log(data.medprice) - np.log(data.low))\n",
    "\n",
    "def MedBodyPriceToLowLog(dataset):\n",
    "    data = dataset.copy()\n",
    "    data[\"medbodyprice\"] = (data.open + data.close) / 2\n",
    "    return pd.Series(np.log(data.medbodyprice) - np.log(data.low))\n",
    "\n",
    "def MedPriceToMedBodyPriceLog(dataset):\n",
    "    data = dataset.copy()\n",
    "    data[\"medprice\"] = (data.high + data.low) / 2\n",
    "    data[\"medbodyprice\"] = (data.open + data.close) / 2\n",
    "    return pd.Series(np.log(data.medprice) - np.log(data.medbodyprice))\n",
    "\n",
    "def MedPriceToMedPriceLog(dataset):\n",
    "    data = dataset.copy()\n",
    "    data[\"medprice\"] = (data.high + data.low) / 2\n",
    "    return pd.Series(np.log(data.medprice) - np.log(data.medprice.shift(1)))\n",
    "\n",
    "def MedBodyPriceToMedBodyPriceLog(dataset):\n",
    "    data = dataset.copy()\n",
    "    data[\"medbodyprice\"] = (data.open + data.close) / 2\n",
    "    return pd.Series(np.log(data.medbodyprice) - np.log(data.medbodyprice).shift(1))\n",
    "\n",
    "def RangeLog(dataset):\n",
    "    data = dataset.copy()\n",
    "    return pd.Series(np.log(data.high) - np.log(data.low))\n",
    "\n",
    "def BodyLog(dataset):\n",
    "    data = dataset.copy()\n",
    "    return pd.Series(np.log(data.close) - np.log(data.open))\n",
    "\n",
    "def BodyToRangeLog(dataset):\n",
    "    data = dataset.copy()\n",
    "    data[\"body\"] = (data.close - data.open) \n",
    "    data[\"range\"] = (data.high - data.low) \n",
    "    return pd.Series(np.log(data.body) - np.log(data.range))\n",
    "\n",
    "def daily_high(dataframe, delay):\n",
    "    df = dataframe.copy()\n",
    "    df[\"rule\"] = np.where(df[\"day\"] != df[\"day\"].shift(1),1,0)\n",
    "    df[\"grouper\"] = df.rule.cumsum()\n",
    "    indexes = df[df.rule == 1].index\n",
    "    service = pd.DataFrame(df[\"high\"].groupby(df[\"grouper\"]).max().shift(delay))\n",
    "    service.set_index(indexes, inplace=True)  \n",
    "    field_name = \"daily_high\" + \"_\" + str(delay)\n",
    "    service.columns = [field_name]\n",
    "    df1 = pd.concat([df, service], axis = 1)\n",
    "    df1[field_name] = df1[field_name].fillna(method = \"ffill\")\n",
    "    return df1[field_name]\n",
    "\n",
    "def weekly_high(dataframe, delay):\n",
    "    df = dataframe.copy()\n",
    "    df[\"rule\"] = np.where(df[\"week\"] != df[\"week\"].shift(1),1,0)\n",
    "    df[\"grouper\"] = df.rule.cumsum()\n",
    "    indexes = df[df.rule == 1].index\n",
    "    service = pd.DataFrame(df[\"high\"].groupby(df[\"grouper\"]).max().shift(delay))\n",
    "    service.set_index(indexes, inplace=True)  \n",
    "    field_name = \"daily_high\" + \"_\" + str(delay)\n",
    "    service.columns = [field_name]\n",
    "    df1 = pd.concat([df, service], axis = 1)\n",
    "    df1[field_name] = df1[field_name].fillna(method = \"ffill\")\n",
    "    return df1[field_name]\n",
    "    \n",
    "def daily_low(dataframe, delay):\n",
    "    df = dataframe.copy()\n",
    "    df[\"rule\"] = np.where(df[\"day\"] != df[\"day\"].shift(1),1,0)\n",
    "    df[\"grouper\"] = df.rule.cumsum()\n",
    "    indexes = df[df.rule == 1].index\n",
    "    service = pd.DataFrame(df[\"low\"].groupby(df[\"grouper\"]).min().shift(delay))\n",
    "    service.set_index(indexes, inplace=True)  \n",
    "    field_name = \"daily_low\" + \"_\" + str(delay)\n",
    "    service.columns = [field_name]\n",
    "    df1 = pd.concat([df, service], axis = 1)\n",
    "    df1[field_name] = df1[field_name].fillna(method = \"ffill\")\n",
    "    return df1[field_name]\n",
    "\n",
    "def weekly_low(dataframe, delay):\n",
    "    df = dataframe.copy()\n",
    "    df[\"rule\"] = np.where(df[\"week\"] != df[\"week\"].shift(1),1,0)\n",
    "    df[\"grouper\"] = df.rule.cumsum()\n",
    "    indexes = df[df.rule == 1].index\n",
    "    service = pd.DataFrame(df[\"low\"].groupby(df[\"grouper\"]).min().shift(delay))\n",
    "    service.set_index(indexes, inplace=True)  \n",
    "    field_name = \"daily_low\" + \"_\" + str(delay)\n",
    "    service.columns = [field_name]\n",
    "    df1 = pd.concat([df, service], axis = 1)\n",
    "    df1[field_name] = df1[field_name].fillna(method = \"ffill\")\n",
    "    return df1[field_name]\n",
    "    \n",
    "def daily_open(dataframe, delay):\n",
    "    df = dataframe.copy()\n",
    "    df[\"rule\"] = np.where(df[\"day\"] != df[\"day\"].shift(1),1,0)\n",
    "    df[\"grouper\"] = df.rule.cumsum()\n",
    "    indexes = df[df.rule == 1].index\n",
    "    service = pd.DataFrame(df[\"open\"].groupby(df[\"grouper\"]).first().shift(delay))\n",
    "    service.set_index(indexes, inplace=True)  \n",
    "    field_name = \"daily_open\" + \"_\" + str(delay)\n",
    "    service.columns = [field_name]\n",
    "    df1 = pd.concat([df, service], axis = 1)\n",
    "    df1[field_name] = df1[field_name].fillna(method = \"ffill\")\n",
    "    return df1[field_name]\n",
    "\n",
    "def weekly_open(dataframe, delay):\n",
    "    df = dataframe.copy()\n",
    "    df[\"rule\"] = np.where(df[\"week\"] != df[\"week\"].shift(1),1,0)\n",
    "    df[\"grouper\"] = df.rule.cumsum()\n",
    "    indexes = df[df.rule == 1].index\n",
    "    service = pd.DataFrame(df[\"open\"].groupby(df[\"grouper\"]).first().shift(delay))\n",
    "    service.set_index(indexes, inplace=True)  \n",
    "    field_name = \"daily_open\" + \"_\" + str(delay)\n",
    "    service.columns = [field_name]\n",
    "    df1 = pd.concat([df, service], axis = 1)\n",
    "    df1[field_name] = df1[field_name].fillna(method = \"ffill\")\n",
    "    return df1[field_name]\n",
    "    \n",
    "def daily_close(dataframe, delay):\n",
    "    df = dataframe.copy()\n",
    "    df[\"rule\"] = np.where(df[\"day\"] != df[\"day\"].shift(1),1,0)\n",
    "    df[\"grouper\"] = df.rule.cumsum()\n",
    "    indexes = df[df.rule == 1].index\n",
    "    service = pd.DataFrame(df[\"close\"].groupby(df[\"grouper\"]).last().shift(delay))\n",
    "    service.set_index(indexes, inplace=True)  \n",
    "    field_name = \"daily_close\" + \"_\" + str(delay)\n",
    "    service.columns = [field_name]\n",
    "    df1 = pd.concat([df, service], axis = 1)\n",
    "    df1[field_name] = df1[field_name].fillna(method = \"ffill\")\n",
    "    return df1[field_name]\n",
    "\n",
    "def weekly_close(dataframe, delay):\n",
    "    df = dataframe.copy()\n",
    "    df[\"rule\"] = np.where(df[\"week\"] != df[\"week\"].shift(1),1,0)\n",
    "    df[\"grouper\"] = df.rule.cumsum()\n",
    "    indexes = df[df.rule == 1].index\n",
    "    service = pd.DataFrame(df[\"close\"].groupby(df[\"grouper\"]).last().shift(delay))\n",
    "    service.set_index(indexes, inplace=True)  \n",
    "    field_name = \"daily_close\" + \"_\" + str(delay)\n",
    "    service.columns = [field_name]\n",
    "    df1 = pd.concat([df, service], axis = 1)\n",
    "    df1[field_name] = df1[field_name].fillna(method = \"ffill\")\n",
    "    return df1[field_name]\n",
    "\n",
    "def daily_close_sma(dataframe, delay, period):\n",
    "    df = dataframe.copy()\n",
    "    df[\"rule\"] = np.where(df[\"day\"] != df[\"day\"].shift(1),1,0)\n",
    "    df[\"grouper\"] = df.rule.cumsum()\n",
    "    indexes = df[df.rule == 1].index\n",
    "    service = pd.DataFrame(df[\"close\"].groupby(df[\"grouper\"]).last().shift(delay))\n",
    "    service.set_index(indexes, inplace=True)  \n",
    "    field_name = \"daily_close\" + \"_\" + str(delay)\n",
    "    service.columns = [field_name]\n",
    "    sma_field_name = \"daily_close_sma\" + \"_\" + str(delay)\n",
    "    service[sma_field_name] = service[field_name].rolling(period).mean()\n",
    "    df1 = pd.concat([df, service], axis = 1)\n",
    "    df1[sma_field_name] = df1[sma_field_name].fillna(method = \"ffill\")\n",
    "    return df1[sma_field_name]\n",
    "\n",
    "def session_high(dataframe, session_hour, session_minute, delay):\n",
    "    df = dataframe.copy()\n",
    "    df[\"rule\"] = np.where((df[\"hour\"] == session_hour) & (df[\"minute\"] == session_minute),1,0)\n",
    "    df[\"grouper\"] = df.rule.cumsum()\n",
    "    df = df[df.grouper > 0] #innesto per cancellare i record precedenti al primo trigger\n",
    "    indexes = df[df.rule == 1].index\n",
    "    service = pd.DataFrame(df[\"high\"].groupby(df[\"grouper\"]).max().shift(delay))#.dropna()\n",
    "    service.set_index(indexes, inplace = True)  \n",
    "    field_name = \"session_high\" + \"_\" + str(delay)\n",
    "    service.columns = [field_name]\n",
    "    df1 = pd.concat([df, service], axis = 1)\n",
    "    df1[field_name] = df1[field_name].fillna(method = \"ffill\")\n",
    "    return df1[field_name]\n",
    "\n",
    "def session_low(dataframe, session_hour, session_minute, delay):\n",
    "    df = dataframe.copy()\n",
    "    df[\"rule\"] = np.where((df[\"hour\"] == session_hour) & (df[\"minute\"] == session_minute),1,0)    \n",
    "    df[\"grouper\"] = df.rule.cumsum()\n",
    "    df = df[df.grouper > 0] #innesto per cancellare i record precedenti al primo trigger\n",
    "    indexes = df[df.rule == 1].index\n",
    "    service = pd.DataFrame(df[\"low\"].groupby(df[\"grouper\"]).min().shift(delay))\n",
    "    service.set_index(indexes, inplace=True)  \n",
    "    field_name = \"session_low\" + \"_\" + str(delay)\n",
    "    service.columns = [field_name]\n",
    "    df1 = pd.concat([df, service], axis = 1)\n",
    "    df1[field_name] = df1[field_name].fillna(method = \"ffill\")\n",
    "    return df1[field_name]\n",
    "\n",
    "def session_open(dataframe, session_hour, session_minute, delay):\n",
    "    df = dataframe.copy()\n",
    "    df[\"rule\"] = np.where((df[\"hour\"] == session_hour) & (df[\"minute\"] == session_minute),1,0)  \n",
    "    df[\"grouper\"] = df.rule.cumsum()\n",
    "    df = df[df.grouper > 0] #innesto per cancellare i record precedenti al primo trigger\n",
    "    indexes = df[df.rule == 1].index\n",
    "    service = pd.DataFrame(df[\"open\"].groupby(df[\"grouper\"]).first().shift(delay))\n",
    "    service.set_index(indexes, inplace=True)  \n",
    "    field_name = \"session_open\" + \"_\" + str(delay)\n",
    "    service.columns = [field_name]\n",
    "    df1 = pd.concat([df, service], axis = 1)\n",
    "    df1[field_name] = df1[field_name].fillna(method = \"ffill\")\n",
    "    return df1[field_name]\n",
    "       \n",
    "def session_close(dataframe, session_hour, session_minute, delay):\n",
    "    df = dataframe.copy()\n",
    "    df[\"rule\"] = np.where((df[\"hour\"] == session_hour) & (df[\"minute\"] == session_minute),1,0) \n",
    "    df[\"grouper\"] = df.rule.cumsum()\n",
    "    df = df[df.grouper > 0] #innesto per cancellare i record precedenti al primo trigger\n",
    "    indexes = df[df.rule == 1].index\n",
    "    service = pd.DataFrame(df[\"close\"].groupby(df[\"grouper\"]).last().shift(delay))\n",
    "    service.set_index(indexes, inplace=True)  \n",
    "    field_name = \"session_close\" + \"_\" + str(delay)\n",
    "    service.columns = [field_name]\n",
    "    df1 = pd.concat([df, service], axis = 1)\n",
    "    df1[field_name] = df1[field_name].fillna(method = \"ffill\")\n",
    "    return df1[field_name]\n",
    "\n",
    "def session_close_sma(dataframe, session_hour, session_minute, delay, period):\n",
    "    df = dataframe.copy()\n",
    "    df[\"rule\"] = np.where((df[\"hour\"] == session_hour) & (df[\"minute\"] == session_minute),1,0) \n",
    "    df[\"grouper\"] = df.rule.cumsum()\n",
    "    df = df[df.grouper > 0] #innesto per cancellare i record precedenti al primo trigger\n",
    "    indexes = df[df.rule == 1].index\n",
    "    service = pd.DataFrame(df[\"close\"].groupby(df[\"grouper\"]).last().shift(delay))\n",
    "    service.set_index(indexes, inplace=True)  \n",
    "    field_name = \"session_close\" + \"_\" + str(delay)\n",
    "    service.columns = [field_name]\n",
    "    sma_field_name = \"session_close_sma\" + \"_\" + str(delay)\n",
    "    service[sma_field_name] = service[field_name].rolling(period).mean()\n",
    "    df1 = pd.concat([df, service], axis = 1)\n",
    "    df1[sma_field_name] = df1[sma_field_name].fillna(method = \"ffill\")\n",
    "    return df1[sma_field_name]\n",
    "\n",
    "def adx(data,period):\n",
    "    \"\"\"\n",
    "    Function to calculate average directional index for a period of 14 days\n",
    "    Inputs: dataframe of prices [\"open\",\"high\",\"low\",\"close\"]\n",
    "    Output: adx\n",
    "    \"\"\"\n",
    "    data[\"+DM\"] = np.where((data.high - data.high.shift(1) > data.low - data.low.shift(1)) & (data.high - data.high.shift(1)>0),\\\n",
    "                          data.high - data.high.shift(1),0)\n",
    "    data[\"-DM\"] = np.where((data.high - data.high.shift(1) < data.low - data.low.shift(1)) & (data.low - data.low.shift(1)>0),\\\n",
    "                          data.low - data.low.shift(1),0)\n",
    "    data[\"m1\"] = data.high - data.low\n",
    "    data[\"m2\"] = abs(data.high - data.close.shift(1))\n",
    "    data[\"m3\"] = abs(data.low - data.close.shift(1))\n",
    "    data[\"TR\"] = data[[\"m1\", \"m2\", \"m3\"]].max(axis = 1)\n",
    "    data[\"+DM14\"] = data[\"+DM\"].rolling(period).sum()\n",
    "    data[\"-DM14\"] = data[\"-DM\"].rolling(period).sum()\n",
    "    data[\"TR14\"] = data[\"TR\"].rolling(period).sum()\n",
    "    data[\"+DI\"] = round(((data[\"+DM14\"]/data[\"TR14\"])*100), 0)\n",
    "    data[\"-DI\"] = round(((data[\"-DM14\"]/data[\"TR14\"])*100), 0)\n",
    "    data[\"DX\"] = round(abs((data[\"+DI\"]-data[\"-DI\"])/(data[\"+DI\"]+data[\"-DI\"])),0)\n",
    "    data[\"ADX\"] = (data.DX.rolling(period).mean())*100\n",
    "    return data.ADX\n",
    "\n",
    "def trix(data,period):\n",
    "    \"\"\"\n",
    "    Function to calculate triple exponential moving average\n",
    "    Inputs: dataframe of prices [\"open\",\"high\",\"low\",\"close\"]\n",
    "    Output: trix\n",
    "    \"\"\"\n",
    "    data[\"MOV_1\"] = data.close.ewm(span=period, adjust=False).mean()\n",
    "    data[\"MOV_2\"] = data.MOV_1.ewm(span=period, adjust=False).mean()\n",
    "    data[\"MOV_3\"] = data.MOV_2.ewm(span=period, adjust=False).mean()\n",
    "    data[\"TRIX\"] = ((data.MOV_3 - data.MOV_3.shift(1))/data.MOV_3.shift(1))*100\n",
    "    return data.TRIX\n",
    "\n",
    "def Keltner_Channel(df, kc_lookback, multiplier, atr_lookback):\n",
    "    high=df.high\n",
    "    low=df.low\n",
    "    close=df.close\n",
    "    tr1 = pd.DataFrame(high - low)\n",
    "    tr2 = pd.DataFrame(abs(high - close.shift()))\n",
    "    tr3 = pd.DataFrame(abs(low - close.shift()))\n",
    "    frames = [tr1, tr2, tr3]\n",
    "    tr = pd.concat(frames, axis = 1, join = 'inner').max(axis = 1)\n",
    "    atr = tr.ewm(alpha = 1/atr_lookback).mean()\n",
    "    \n",
    "    kc_middle = close.ewm(kc_lookback).mean()\n",
    "    kc_upper = close.ewm(kc_lookback).mean() + multiplier * atr\n",
    "    kc_lower = close.ewm(kc_lookback).mean() - multiplier * atr\n",
    "    \n",
    "    return kc_upper, kc_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17890964-f7d6-4566-88c9-6b62ef1a9bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e062096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "import random\n",
    "\n",
    "np.random.seed(None)\n",
    "\n",
    "def rnd_gene():\n",
    "    return np.random.randint(0, rules.shape[0] - 1)\n",
    "\n",
    "\n",
    "# TODO: create numpy array directly\n",
    "# build a random dna\n",
    "def rnd_dna():\n",
    "    dna = []\n",
    "    for _ in range(DNA_SIZE):\n",
    "        dna.append(rnd_gene())\n",
    "    return np.array(dna)\n",
    "\n",
    "\n",
    "# build first population at random\n",
    "def first_generation():\n",
    "    population = []\n",
    "    for _ in range(POPULATION_SIZE):\n",
    "        population.append(rnd_dna())\n",
    "    return population\n",
    "\n",
    "def dna_formula(dna):\n",
    "    formula = \"\"\n",
    "    for gene in dna:\n",
    "        if len(formula) != 0:\n",
    "            formula += \" & \"\n",
    "        formula += \"rules[\"+str(gene)+\"]\"\n",
    "    return formula\n",
    "\n",
    "\"\"\"def dna_formula(dna):\n",
    "    formula = \"\"\n",
    "    for gene in dna:\n",
    "        if len(formula) != 0:\n",
    "            formula += \" & \"\n",
    "        formula += rule_formulas[gene]\n",
    "    return formula\"\"\"\n",
    "\n",
    "def sort_population(population):\n",
    "    scores = list(map(fitness, population))\n",
    "    sorted_population = [dna for _, dna in sorted(zip(scores, population), key=lambda pair: pair[0], reverse=True)]\n",
    "    return sorted_population\n",
    "\n",
    "def mutate(dna1):\n",
    "    dna = dna1.copy()\n",
    "    dna[np.random.randint(DNA_SIZE)] = rnd_gene()\n",
    "    return dna\n",
    "\n",
    "\n",
    "# cross two dnas\n",
    "def cross(dna1, dna2):\n",
    "    dna = []\n",
    "    for i in range(DNA_SIZE):\n",
    "        if np.random.randint(2) == 0:\n",
    "            dna.append(dna1[i])\n",
    "        else:\n",
    "            dna.append(dna2[i])\n",
    "    return np.array(dna)\n",
    "\n",
    "\n",
    "\n",
    "def next_generation(population):\n",
    "\n",
    "    n = len(population)\n",
    "    n_best = int(n * BEST_DNA_RATIO)\n",
    "    n_cross = int(n * CROSS_DNA_RATIO)\n",
    "    n_mutate = n - n_best - n_cross\n",
    "\n",
    "    new_population = np.empty_like(population)\n",
    "\n",
    "    # copy best dnas\n",
    "    for i in range(n_best):\n",
    "        new_population[i] = population[i]\n",
    "\n",
    "    # cross\n",
    "    for i in range(n_cross):\n",
    "        #dna1 = population[np.random.randint(n_best)]\n",
    "        #dna2 = population[np.random.randint(n_best)]\n",
    "        dna1 = population[np.random.randint(n)]\n",
    "        dna2 = population[np.random.randint(n)]\n",
    "        dna = cross(dna1, dna2)\n",
    "        new_population[n_best + i] = dna\n",
    "\n",
    "    # mutate\n",
    "    for i in range(n_mutate):\n",
    "        #dna1 = population[np.random.randint(n_best)]\n",
    "        dna1 = population[np.random.randint(n_best)]\n",
    "        dna = mutate(dna1)\n",
    "        new_population[n_best + n_cross + i] = dna\n",
    "\n",
    "    return new_population\n",
    "\n",
    "\n",
    "def evolution():\n",
    "    np.random.seed(None)\n",
    "    best_dnas = []\n",
    "    population = first_generation()\n",
    "    population = sort_population(population)\n",
    "    best_dnas.append(population[0])\n",
    "    print_best(population, 0)\n",
    "\n",
    "    # evolve for NUM_GENERATIONS generations\n",
    "    for i in tqdm(range(1, NUM_GENERATIONS)):\n",
    "        population = next_generation(population)\n",
    "        population = sort_population(population)\n",
    "        best_dnas.append(population[0])\n",
    "        print_best(population, i)\n",
    "    \n",
    "    return best_dnas\n",
    "\n",
    "def dna_formula_dataframe(dna):\n",
    "    formula = \"\"\n",
    "    for gene in dna:\n",
    "        if len(formula) != 0:\n",
    "            formula += \" & \"\n",
    "        formula += \"dataset_ind.iloc[:,\"+str(gene)+\"]\"\n",
    "    return formula\n",
    "\n",
    "\n",
    "\n",
    "def target(direzione,_search,dataset):\n",
    "\n",
    "    price=dataset.open.copy()\n",
    "    t_m=[]\n",
    "    t_m=price.shift(-_search)-price\n",
    "    t_m=remove_outlier(t_m)\n",
    "    t_m=t_m.shift(-1)\n",
    "    t_m=t_m.fillna(0)\n",
    "    if direzione == \"long\":\n",
    "        t_m=np.array(t_m * bigpointvalue)\n",
    "    else:\n",
    "        t_m=t_m*(-1)\n",
    "        t_m=np.array(t_m * bigpointvalue)\n",
    "    return t_m\n",
    "\n",
    "def check_level(level,direzione,tipo_ordine):\n",
    "    \n",
    "    level=eval(level)\n",
    "    \n",
    "    if (direzione == \"long\") & (tipo_ordine == \"stop\"):  \n",
    "        level_check = (history.high >= level)\n",
    "        \n",
    "    if (direzione == \"short\") & (tipo_ordine == \"stop\"):    \n",
    "        level_check = (history.low <= level)\n",
    "        \n",
    "    if (direzione == \"long\") & (tipo_ordine == \"limit\"):  \n",
    "        level_check = (history.low <= level)\n",
    "        \n",
    "    if (direzione == \"short\") & (tipo_ordine == \"limit\"):    \n",
    "        level_check = (history.high >= level)\n",
    "                            \n",
    "    return level_check.T.values\n",
    "\n",
    "\n",
    "\n",
    "def kestner_ratio_mod(operations):\n",
    "\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt \n",
    "    from scipy import stats\n",
    "    \n",
    "    equity = operations.cumsum()\n",
    "    index = np.array(np.arange(1,equity.count() + 1))\n",
    "    \n",
    "    x = index\n",
    "    y = equity\n",
    "    gradient, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
    "    \n",
    "    if std_err != 0 and len(index) > 0:\n",
    "        return round(gradient / (std_err * len(index)),5)\n",
    "    else:\n",
    "        return np.inf\n",
    "    \n",
    "def traduci(x):\n",
    "    tmp= x.split(\" & \")\n",
    "    stringa=\"\"\n",
    "    if len(tmp) >1:\n",
    "        for e1 in range(len(tmp)):\n",
    "            tmp2=eval(tmp[e1]+'.name')\n",
    "            tmp2=\"(\"+tmp2+\")\"\n",
    "            if e1 != (len(tmp)-1):\n",
    "                stringa=stringa+tmp2+\" & \"\n",
    "            else:\n",
    "                stringa=stringa+tmp2\n",
    "    else:\n",
    "        tmp2=eval(tmp[0]+'.name')\n",
    "        stringa=\"(\"+tmp2+\")\"\n",
    "        \n",
    "    return stringa     \n",
    "\n",
    "\"\"\"def fitness(dna0):\n",
    "    \n",
    "    tmp=dna_formula(dna0)\n",
    "    tmp0 = eval(tmp)\n",
    "    tf0 = strategy(tmp0,exit,target_monetario,costi,level_check,tipo_ordine)\n",
    "    tf=pd.Series(index=list(range(len(tf0))),data=tf0)\n",
    "    \n",
    "    if len(tf)>min_n_operazioni:\n",
    "        if ranking == \"avg_trade\":\n",
    "            gain=avg_trade(tf)\n",
    "        if ranking == \"profit_factor\":\n",
    "            gain=profit_factor(tf)\n",
    "        if ranking == \"profit_factor\":\n",
    "            gain=percent_win(tf)\n",
    "        if ranking == \"profit\":\n",
    "            gain=tf.sum()\n",
    "        if gain != np.nan:\n",
    "            return gain\n",
    "        else:\n",
    "            return -100\n",
    "    else:\n",
    "        return -100\n",
    "    \n",
    "# print performance of best dna\n",
    "def print_best(population, gen_num):\n",
    "    best_dna = population[0]    \n",
    "    tmp=dna_formula(best_dna)\n",
    "    tmp=eval(tmp)\n",
    "    tf0 = strategy(tmp,exit,target_monetario,costi,level_check,tipo_ordine)\n",
    "    tf=pd.Series(index=list(range(len(tf0))),data=tf0)\n",
    "    if len(tf)>1:\n",
    "        clear_output(wait=True)\n",
    "        tf.cumsum().plot(figsize=(5,5),label=\"Generazione n°:\"+str(gen_num))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return tf\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a211f029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "@jit(nopython=True)\n",
    "def strategy(tmp,exit,target_monetario,costi,level_check,tipo_ordine,ADD_RULE):\n",
    "    \n",
    "    tmp = tmp & level_check & ADD_RULE \n",
    "    for el in range(len(tmp)):\n",
    "        if tmp[el] == 1:\n",
    "            tmp[el+1:el+exit+1]=0\n",
    "    d=tmp*target_monetario\n",
    "    d[d != 0 ]=d[d != 0 ]-costi\n",
    "    return d\n",
    "\n",
    "@jit(nopython=True)\n",
    "def avg_trade_numba(array):\n",
    "    array=array[array != 0]\n",
    "    avg_t=sum(array)/array.size\n",
    "    return avg_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9427c828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_report_fast(tradelist,\n",
    "                       closed_equity,\n",
    "                       open_equity,\n",
    "                       initial_capital,\n",
    "                       risk_free_rate,\n",
    "                       interactive):\n",
    "    print(\"********************************************************************\")\n",
    "    print(\"*** Performance Report - Copyright 2020 - by Gandalf Project R&D ***\")\n",
    "    print(\"********************************************************************\")\n",
    "    if tradelist.empty:\n",
    "        print(\"\")\n",
    "        print(\"Nessuna operazione registrata!\")\n",
    "        return\n",
    "    else: \n",
    "        print(\"\")\n",
    "        print(\"Compound Annual Growth Rate CAGR: \" + str(cagr(tradelist.operations,initial_capital)) + \" (capital = \" + str(initial_capital) + \")\")\n",
    "        print(\"Annual Return: \" + str(annual_return(tradelist.operations,initial_capital)) + \" (capital = \" + str(initial_capital) + \")\")\n",
    "        print(\"\")\n",
    "        print(\"Calmar Ratio (yearly):\", calmar_ratio(tradelist.operations))\n",
    "        print(\"Sharpe Ratio: \" + str(sharpe_ratio(tradelist.operations, initial_capital, risk_free_rate)) + \" (capital = \" + str(initial_capital) + \", risk free rate = \" + str(risk_free_rate) + \")\")\n",
    "        print(\"Sortino Ratio: \" + str(sortino_ratio(tradelist.operations, initial_capital, risk_free_rate / 12)) + \" (capital = \" + str(initial_capital) + \", risk free rate = \" + str(risk_free_rate / 12) + \")\")\n",
    "        print(\"Omega Ratio: \" + str(omega_ratio(tradelist.operations,100)) + \" (threshold = 100)\") \n",
    "        print(\"Kestner Ratio:\", kestner_ratio(tradelist.operations))\n",
    "        print(\"\")\n",
    "        print(\"Profit:                  \", int(tradelist.operations.sum()))\n",
    "        print(\"Operations:              \", operation_number(tradelist.operations))\n",
    "        print(\"Average Trade:           \", avg_trade(tradelist.operations))\n",
    "        print(\"\")\n",
    "        print(\"Profit Factor:           \", profit_factor(tradelist.operations))\n",
    "        print(\"Gross Profit:            \", gross_profit(tradelist.operations))\n",
    "        print(\"Gross Loss:              \", gross_loss(tradelist.operations))\n",
    "        print(\"\")\n",
    "        print(\"Percent Winning Trades:  \", percent_win(tradelist.operations))\n",
    "        print(\"Percent Losing Trades:   \", round(100 - percent_win(tradelist.operations),2))\n",
    "        print(\"Reward Risk Ratio:       \", reward_risk_ratio(tradelist.operations))\n",
    "        print(\"\")\n",
    "        print(\"Max Gain:                \", max_gain(tradelist.operations), \" in date \", max_gain_date(tradelist.operations))\n",
    "        print(\"Average Gain:            \", avg_gain(tradelist.operations))\n",
    "        print(\"Max Loss:                \", max_loss(tradelist.operations), \" in date \", max_loss_date(tradelist.operations))\n",
    "        print(\"Average Loss:            \", avg_loss(tradelist.operations))\n",
    "        print(\"\")\n",
    "        print(\"Avg Delay Between Peaks: \", avg_delay_between_peaks(tradelist.operations.cumsum()))\n",
    "        print(\"Max Delay Between Peaks: \", max_delay_between_peaks(tradelist.operations.cumsum()))\n",
    "        print(\"\")\n",
    "        print(\"Trades Standard Deviation: \", tradelist.operations.std())\n",
    "        print(\"Equity Standard Deviation: \", tradelist.operations.cumsum().std())\n",
    "        print(\"\")\n",
    "        print(\"Avg Open Draw Down:      \", avgdrawdown_nozero(tradelist.operations.cumsum()))\n",
    "        print(\"Max Open Draw Down:      \", max_draw_down(tradelist.operations.cumsum()))\n",
    "        print(\"\")\n",
    "        print(\"Avg Closed Draw Down:    \", avgdrawdown_nozero(tradelist.operations.cumsum()))\n",
    "        print(\"Max Closed Draw Down:    \", max_draw_down(tradelist.operations.cumsum()))\n",
    "        print(\"\")\n",
    "        print(\"Draw Down Statistics: \", drawdown_statistics(tradelist.operations.cumsum()))\n",
    "        print(\"\")\n",
    "        \n",
    "        print(\"Operations Statistics:\\n\")\n",
    "\n",
    "        if interactive == False:\n",
    "            plot_equity(tradelist.operations.cumsum(),\"green\")\n",
    "            plot_drawdown(tradelist.operations.cumsum(),\"red\")\n",
    "            plot_annual_histogram(tradelist.operations)\n",
    "            plot_monthly_bias_histogram(tradelist.operations)\n",
    "            plot_equity_heatmap(tradelist.operations,False)\n",
    "\n",
    "        else:\n",
    "            plot_equity_interactive(tradelist.operations.cumsum(),\"green\")\n",
    "            plot_drawdown_interactive(tradelist.operations.cumsum())\n",
    "            plot_annual_histogram_interactive(tradelist.operations)\n",
    "            plot_monthly_bias_histogram_interactive(tradelist.operations)\n",
    "            plot_equity_heatmap_interactive(tradelist.operations,False)\n",
    "\n",
    "        return\n",
    "    \n",
    "def info_simboli():\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    valori=pd.read_csv(\"Markets_Specifications_Live.csv\",sep=\",\",encoding='cp1252')\n",
    "    valori.reset_index(inplace=True)\n",
    "    valori.Exchange_Time=np.where(valori.Exchange_Time==-6,\"CST6CDT\",valori.Exchange_Time)\n",
    "    valori.Exchange_Time=np.where(valori.Exchange_Time==\"-5\",\"EST5EDT\",valori.Exchange_Time)\n",
    "    valori.Exchange_Time=np.where(valori.Exchange_Time==\"1\",\"CET\",valori.Exchange_Time)\n",
    "    valori[\"Simbolo\"]=valori.Physical\n",
    "    display(valori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2841e7f-6349-48f3-9a79-dedf76c822a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def level_to_condition(enter_level,history,tipo_ordine,direzione):\n",
    "    enter_level=eval(enter_level)\n",
    "    history[\"level_check\"] = [True]*len(enter_level)\n",
    "    if (tipo_ordine == \"stop\") | (tipo_ordine == \"limit\"):\n",
    "        if (direzione == \"long\") & (tipo_ordine == \"stop\"):  \n",
    "            history[\"level_check\"] = (history.high >= enter_level)\n",
    "        if (direzione == \"short\") & (tipo_ordine == \"stop\"):    \n",
    "            history[\"level_check\"] = (history.low <= enter_level)\n",
    "        if (direzione == \"long\") & (tipo_ordine == \"limit\"):  \n",
    "            history[\"level_check\"] = (history.low <= enter_level)\n",
    "        if (direzione == \"short\") & (tipo_ordine == \"limit\"):    \n",
    "            history[\"level_check\"] = (history.high >= enter_level)\n",
    "\n",
    "    return history[\"level_check\"]\n",
    "\n",
    "def check_stoploss(SL,history,enter_level,BIGPOINTVALUE,direzione):\n",
    "\n",
    "    if (SL != 0) & (direzione == \"long\") :\n",
    "        return ((history.close - enter_level) * BIGPOINTVALUE) <= -SL\n",
    "    if (SL != 0) & (direzione == \"short\") :\n",
    "        return ((enter_level - history.close ) * BIGPOINTVALUE) <= -SL\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def check_takeprofit(TP,history,enter_level,BIGPOINTVALUE,direzione):\n",
    "\n",
    "    if (TP != 0) & (direzione == \"long\"):\n",
    "        return ((history.close - enter_level) * BIGPOINTVALUE) >= TP\n",
    "    if (TP != 0) & (direzione == \"short\"):\n",
    "        return ((enter_level - history.close) * BIGPOINTVALUE) >= TP\n",
    "    else:\n",
    "        return False    \n",
    "    \n",
    "def check_add_rule(ADD_RULE_CONDITION,history):\n",
    "    if str(ADD_RULE_CONDITION) == \"False\":\n",
    "        return np.array([True]*len(history.iloc[:,0]))\n",
    "    else:\n",
    "        history[\"add_rule_check\"]=eval(ADD_RULE_CONDITION)\n",
    "        return np.array(history[\"add_rule_check\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ac1179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def strategy_numba(idx,bigpointvalue,tick,costi,direzione,exit_bars,level,tipo_ordine,TP,SL,DATAFRAME,ADD_RULE_CONDITION):\n",
    "    \n",
    "    history=DATAFRAME.copy()\n",
    "    \n",
    "    history[\"signal\"] = eval(idx) \n",
    "    \n",
    "    history[\"level_to_condition\"] = level_to_condition(level,history,tipo_ordine,direzione)\n",
    "    \n",
    "    history[\"add_condition\"] = check_add_rule(ADD_RULE_CONDITION,history)\n",
    "        \n",
    "    history[\"level\"] = eval(level)\n",
    "    \n",
    "    WRITELOG = False\n",
    "\n",
    "    NAME = \"Tester\"\n",
    "\n",
    "    COSTS_FIXED = costi/2\n",
    "    # 10 for 10 $ for each transaction (2 * fee each operation)\n",
    "    COSTS_VARIABLE = 0.0\n",
    "    # 1.0 for 1% (2 * fee each operation)\n",
    "    COSTS_PERSHARES = 0.0 \n",
    "    # 0.01 for 1 cent for each shares traded (2 * fee each operation)\n",
    "\n",
    "    INSTRUMENT = 2 # 1: equity, 2: future, 3: crypto/forex\n",
    "    TICK = tick\n",
    "    BIGPOINTVALUE = bigpointvalue\n",
    "\n",
    "    ORDER_TYPE = \"market\"\n",
    "\n",
    "    QUANTITY = 1\n",
    "    MARGIN_PERCENT = 20 # margin to use in capital calculation\n",
    "\n",
    "    MAX_INTRADAY_OPERATIONS = 100\n",
    "\n",
    "    TIME_EXIT = exit_bars  # 0 to disable\n",
    "    TIME_EXIT_LOSS = 0  # 0 to disable\n",
    "    TIME_EXIT_GAIN = 0  # 0 to disable\n",
    "\n",
    "    MONEY_STOPLOSS = 0 # 0 to disable\n",
    "    MONEY_TARGET = 0  # 0 to disable\n",
    "\n",
    "    MIN_MONEY_PERCENT_STOPLOSS = 0\n",
    "    PERCENT_STOPLOSS = 0.0  # 0 to disable\n",
    "\n",
    "    MIN_MONEY_PERCENT_TARGET = 0\n",
    "    PERCENT_TARGET = 0.0  # 0 to disable      \n",
    "    \n",
    "    enter_level = history[\"level\"]\n",
    "        \n",
    "    enter_rules = history[\"signal\"] & history[\"level_to_condition\"] & history[\"add_condition\"]\n",
    "    \n",
    "    exit_rules = False \n",
    "    \n",
    "    exit_rules_loss = check_stoploss(SL,history,enter_level,BIGPOINTVALUE,direzione)\n",
    "\n",
    "    exit_rules_gain = check_takeprofit(TP,history,enter_level,BIGPOINTVALUE,direzione) \n",
    "\n",
    "    target_level =  0  # 0 to disable\n",
    "\n",
    "    stop_level = 0  # 0 to disable\n",
    "\n",
    "    ##############################################################################################################################################################################################################################\n",
    "    if direzione == \"long\":\n",
    "    \n",
    "        DIRECTION = \"long\"\n",
    "\n",
    "        tradelist, open_equity, closed_equity, operation_equity = apply_trading_system(history, INSTRUMENT, QUANTITY, MARGIN_PERCENT, BIGPOINTVALUE, TICK, DIRECTION,\n",
    "                                                                                       COSTS_FIXED, COSTS_VARIABLE, COSTS_PERSHARES, \n",
    "                                                                                       ORDER_TYPE, enter_level, enter_rules, MAX_INTRADAY_OPERATIONS, \n",
    "                                                                                       exit_rules, exit_rules_loss, exit_rules_gain,\n",
    "                                                                                       target_level, stop_level,\n",
    "                                                                                       TIME_EXIT, TIME_EXIT_LOSS, TIME_EXIT_GAIN, \n",
    "                                                                                       MONEY_STOPLOSS, MONEY_TARGET, \n",
    "                                                                                       PERCENT_STOPLOSS, MIN_MONEY_PERCENT_STOPLOSS, \n",
    "                                                                                       PERCENT_TARGET, MIN_MONEY_PERCENT_TARGET, WRITELOG)\n",
    "        if len(tradelist)>0:\n",
    "            return tradelist.operations\n",
    "        else:\n",
    "            tradelist, open_equity, closed_equity, operation_equity = apply_trading_system(history.iloc[:-1], INSTRUMENT, QUANTITY, MARGIN_PERCENT, BIGPOINTVALUE, TICK, DIRECTION,\n",
    "                                                                               COSTS_FIXED, COSTS_VARIABLE, COSTS_PERSHARES, \n",
    "                                                                               ORDER_TYPE, enter_level, enter_rules, MAX_INTRADAY_OPERATIONS, \n",
    "                                                                               exit_rules, exit_rules_loss, exit_rules_gain,\n",
    "                                                                               target_level, stop_level,\n",
    "                                                                               TIME_EXIT, TIME_EXIT_LOSS, TIME_EXIT_GAIN, \n",
    "                                                                               MONEY_STOPLOSS, MONEY_TARGET, \n",
    "                                                                               PERCENT_STOPLOSS, MIN_MONEY_PERCENT_STOPLOSS, \n",
    "                                                                               PERCENT_TARGET, MIN_MONEY_PERCENT_TARGET, WRITELOG)\n",
    "            if len(tradelist)>0:\n",
    "                return tradelist.operations\n",
    "            else:\n",
    "                return pd.DataFrame(columns=[\"operations\"])\n",
    "    \n",
    "    ##############################################################################################################################################################################################################################\n",
    "    \n",
    "    if direzione == \"short\":\n",
    "    \n",
    "        DIRECTION = \"short\"\n",
    "\n",
    "        tradelist, open_equity, closed_equity, operation_equity = apply_trading_system(history, INSTRUMENT, QUANTITY, MARGIN_PERCENT, BIGPOINTVALUE, TICK, DIRECTION,\n",
    "                                                                                       COSTS_FIXED, COSTS_VARIABLE, COSTS_PERSHARES, \n",
    "                                                                                       ORDER_TYPE, enter_level, enter_rules, MAX_INTRADAY_OPERATIONS, \n",
    "                                                                                       exit_rules, exit_rules_loss, exit_rules_gain,\n",
    "                                                                                       target_level, stop_level,\n",
    "                                                                                       TIME_EXIT, TIME_EXIT_LOSS, TIME_EXIT_GAIN, \n",
    "                                                                                       MONEY_STOPLOSS, MONEY_TARGET, \n",
    "                                                                                       PERCENT_STOPLOSS, MIN_MONEY_PERCENT_STOPLOSS, \n",
    "                                                                                       PERCENT_TARGET, MIN_MONEY_PERCENT_TARGET, WRITELOG)\n",
    "        if len(tradelist)>0:\n",
    "            return tradelist.operations\n",
    "        else:\n",
    "            tradelist, open_equity, closed_equity, operation_equity = apply_trading_system(history.iloc[:-1], INSTRUMENT, QUANTITY, MARGIN_PERCENT, BIGPOINTVALUE, TICK, DIRECTION,\n",
    "                                                                               COSTS_FIXED, COSTS_VARIABLE, COSTS_PERSHARES, \n",
    "                                                                               ORDER_TYPE, enter_level, enter_rules, MAX_INTRADAY_OPERATIONS, \n",
    "                                                                               exit_rules, exit_rules_loss, exit_rules_gain,\n",
    "                                                                               target_level, stop_level,\n",
    "                                                                               TIME_EXIT, TIME_EXIT_LOSS, TIME_EXIT_GAIN, \n",
    "                                                                               MONEY_STOPLOSS, MONEY_TARGET, \n",
    "                                                                               PERCENT_STOPLOSS, MIN_MONEY_PERCENT_STOPLOSS, \n",
    "                                                                               PERCENT_TARGET, MIN_MONEY_PERCENT_TARGET, WRITELOG)\n",
    "            if len(tradelist)>0:\n",
    "                return tradelist.operations\n",
    "            else:\n",
    "                return pd.DataFrame(columns=[\"operations\"])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8623cc19-6e1b-4c67-b9ca-2810177ef56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def strategy_numba_val(v4,bigpointvalue,tick,stampa,v3,tipo_ordine,v2,direzione,v1,v5):\n",
    "    \n",
    "    \n",
    "    WRITELOG = False\n",
    "\n",
    "    NAME = \"Tester\"\n",
    "\n",
    "    COSTS_FIXED = costi/2\n",
    "    # 10 for 10 $ for each transaction (2 * fee each operation)\n",
    "    COSTS_VARIABLE = 0.0\n",
    "    # 1.0 for 1% (2 * fee each operation)\n",
    "    COSTS_PERSHARES = 0.0 \n",
    "    # 0.01 for 1 cent for each shares traded (2 * fee each operation)\n",
    "\n",
    "    INSTRUMENT = 2 # 1: equity, 2: future, 3: crypto/forex\n",
    "    TICK = tick\n",
    "    BIGPOINTVALUE = bigpointvalue\n",
    "\n",
    "    ORDER_TYPE = \"market\"\n",
    "\n",
    "    QUANTITY = 1\n",
    "    MARGIN_PERCENT = 20 # margin to use in capital calculation\n",
    "\n",
    "    MAX_INTRADAY_OPERATIONS = 100\n",
    "\n",
    "    TIME_EXIT = v1  # 0 to disable\n",
    "    TIME_EXIT_LOSS = 0  # 0 to disable\n",
    "    TIME_EXIT_GAIN = 0  # 0 to disable\n",
    "\n",
    "    MONEY_STOPLOSS = 0 # 0 to disable\n",
    "    MONEY_TARGET = 0  # 0 to disable\n",
    "\n",
    "    MIN_MONEY_PERCENT_STOPLOSS = 0\n",
    "    PERCENT_STOPLOSS = 0.0  # 0 to disable\n",
    "\n",
    "    MIN_MONEY_PERCENT_TARGET = 0\n",
    "    PERCENT_TARGET = 0.0  # 0 to disable\n",
    "\n",
    "    enter_level = v3\n",
    "\n",
    "    enter_rules = v4\n",
    "    \n",
    "    exit_rules = False \n",
    "\n",
    "    exit_rules_loss = check_stoploss(v2,history,enter_level,BIGPOINTVALUE,direzione)\n",
    "\n",
    "    exit_rules_gain = check_takeprofit(v5,history,enter_level,BIGPOINTVALUE,direzione) \n",
    "\n",
    "    target_level =  0  # 0 to disable\n",
    "\n",
    "    stop_level = 0  # 0 to disable\n",
    "\n",
    "    ##############################################################################################################################################################################################################################\n",
    "    if direzione == \"long\":\n",
    "    \n",
    "        DIRECTION = \"long\"\n",
    "\n",
    "        tradelist, open_equity, closed_equity, operation_equity = apply_trading_system(history, INSTRUMENT, QUANTITY, MARGIN_PERCENT, BIGPOINTVALUE, TICK, DIRECTION,\n",
    "                                                                                       COSTS_FIXED, COSTS_VARIABLE, COSTS_PERSHARES, \n",
    "                                                                                       ORDER_TYPE, enter_level, enter_rules, MAX_INTRADAY_OPERATIONS, \n",
    "                                                                                       exit_rules, exit_rules_loss, exit_rules_gain,\n",
    "                                                                                       target_level, stop_level,\n",
    "                                                                                       TIME_EXIT, TIME_EXIT_LOSS, TIME_EXIT_GAIN, \n",
    "                                                                                       MONEY_STOPLOSS, MONEY_TARGET, \n",
    "                                                                                       PERCENT_STOPLOSS, MIN_MONEY_PERCENT_STOPLOSS, \n",
    "                                                                                       PERCENT_TARGET, MIN_MONEY_PERCENT_TARGET, WRITELOG)\n",
    "        if len(tradelist)>0:\n",
    "            return tradelist.operations\n",
    "        else:\n",
    "            tradelist, open_equity, closed_equity, operation_equity = apply_trading_system(history.iloc[:-1], INSTRUMENT, QUANTITY, MARGIN_PERCENT, BIGPOINTVALUE, TICK, DIRECTION,\n",
    "                                                                               COSTS_FIXED, COSTS_VARIABLE, COSTS_PERSHARES, \n",
    "                                                                               ORDER_TYPE, enter_level, enter_rules, MAX_INTRADAY_OPERATIONS, \n",
    "                                                                               exit_rules, exit_rules_loss, exit_rules_gain,\n",
    "                                                                               target_level, stop_level,\n",
    "                                                                               TIME_EXIT, TIME_EXIT_LOSS, TIME_EXIT_GAIN, \n",
    "                                                                               MONEY_STOPLOSS, MONEY_TARGET, \n",
    "                                                                               PERCENT_STOPLOSS, MIN_MONEY_PERCENT_STOPLOSS, \n",
    "                                                                               PERCENT_TARGET, MIN_MONEY_PERCENT_TARGET, WRITELOG)\n",
    "            if len(tradelist)>0:\n",
    "                return tradelist.operations\n",
    "            else:\n",
    "                return pd.DataFrame(columns=[\"operations\"])\n",
    "    \n",
    "    ##############################################################################################################################################################################################################################\n",
    "    \n",
    "    if direzione == \"short\":\n",
    "    \n",
    "        DIRECTION = \"short\"\n",
    "\n",
    "        tradelist, open_equity, closed_equity, operation_equity = apply_trading_system(history, INSTRUMENT, QUANTITY, MARGIN_PERCENT, BIGPOINTVALUE, TICK, DIRECTION,\n",
    "                                                                                       COSTS_FIXED, COSTS_VARIABLE, COSTS_PERSHARES, \n",
    "                                                                                       ORDER_TYPE, enter_level, enter_rules, MAX_INTRADAY_OPERATIONS, \n",
    "                                                                                       exit_rules, exit_rules_loss, exit_rules_gain,\n",
    "                                                                                       target_level, stop_level,\n",
    "                                                                                       TIME_EXIT, TIME_EXIT_LOSS, TIME_EXIT_GAIN, \n",
    "                                                                                       MONEY_STOPLOSS, MONEY_TARGET, \n",
    "                                                                                       PERCENT_STOPLOSS, MIN_MONEY_PERCENT_STOPLOSS, \n",
    "                                                                                       PERCENT_TARGET, MIN_MONEY_PERCENT_TARGET, WRITELOG)\n",
    "        if len(tradelist)>0:\n",
    "            return tradelist.operations\n",
    "        else:\n",
    "            tradelist, open_equity, closed_equity, operation_equity = apply_trading_system(history.iloc[:-1], INSTRUMENT, QUANTITY, MARGIN_PERCENT, BIGPOINTVALUE, TICK, DIRECTION,\n",
    "                                                                               COSTS_FIXED, COSTS_VARIABLE, COSTS_PERSHARES, \n",
    "                                                                               ORDER_TYPE, enter_level, enter_rules, MAX_INTRADAY_OPERATIONS, \n",
    "                                                                               exit_rules, exit_rules_loss, exit_rules_gain,\n",
    "                                                                               target_level, stop_level,\n",
    "                                                                               TIME_EXIT, TIME_EXIT_LOSS, TIME_EXIT_GAIN, \n",
    "                                                                               MONEY_STOPLOSS, MONEY_TARGET, \n",
    "                                                                               PERCENT_STOPLOSS, MIN_MONEY_PERCENT_STOPLOSS, \n",
    "                                                                               PERCENT_TARGET, MIN_MONEY_PERCENT_TARGET, WRITELOG)\n",
    "            if len(tradelist)>0:\n",
    "                return tradelist.operations\n",
    "            else:\n",
    "                return pd.DataFrame(columns=[\"operations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4af1ac-58f9-4543-a9de-c8143a4ea064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_history_name(file_name,dir_history):\n",
    "    import os\n",
    "    storici = os.listdir(dir_history)\n",
    "    for e in storici:\n",
    "        x=e.split(\"_\")[0]\n",
    "        x=x.replace(\"@\",\"\")\n",
    "        if x == file_name:\n",
    "            file_name=e\n",
    "            break\n",
    "    return file_name\n",
    "\n",
    "def last_price_std(valori):\n",
    "    last_price_list=[]\n",
    "    std_list=[]\n",
    "    for e in tqdm(valori.Physical):\n",
    "        file_name=check_history_name(e,dir_history)\n",
    "        os.chdir(dir_history)\n",
    "        if file_name!=e:\n",
    "            df=pd.read_csv(file_name)\n",
    "            l_close=df.close.iloc[-1]\n",
    "            last_price_list.append(l_close)\n",
    "            std_list.append(df.close.std())\n",
    "        else:\n",
    "            last_price_list.append(0)\n",
    "            std_list.append(0)\n",
    "\n",
    "    return last_price_list , std_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865afe9f-01ae-4756-802b-0ef2a9fe4eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def carica_storico_fast(simbolo,uct_offset,type_session,resample_tf,IS,OOS,custom_session_start,custom_session_stop):\n",
    "    import os\n",
    "    import talib as ta\n",
    "      \n",
    "    file_name = check_history_name(simbolo,dir_history)\n",
    "\n",
    "    os.chdir(dir_history)\n",
    "    data = load_data_intraday_fast(file_name)  \n",
    "    print(\"Caricato storico\")\n",
    "    data = data.sort_index(ascending=True)\n",
    "    data.index = data.index.tz_localize('Etc/Zulu')\n",
    "    data.index = data.index.tz_convert(uct_offset )\n",
    "    data.index = data.index.tz_localize(None)\n",
    "    data = data[data!=0]\n",
    "    data = data.iloc[1:-1]\n",
    "    print(\"Resample dei dati\")\n",
    "    if type_session == 1:\n",
    "        dataset=resample_custom_session(\"5min\",resample_tf ,data,custom_session_start,custom_session_stop) \n",
    "    if type_session == 0:    \n",
    "        dataset=resample_standard_session(\"5min\",resample_tf,data)\n",
    "        \n",
    "    dataset = dataset.loc[IS:OOS]\n",
    "    dataset[\"dayofweek\"] = dataset.index.dayofweek\n",
    "    dataset[\"day\"] = dataset.index.day\n",
    "    dataset[\"month\"] = dataset.index.month\n",
    "    dataset[\"year\"] = dataset.index.year\n",
    "    dataset[\"dayofyear\"] = dataset.index.dayofyear\n",
    "    dataset[\"quarter\"] = dataset.index.quarter\n",
    "    dataset[\"hour\"] = dataset.index.hour\n",
    "    dataset[\"minute\"] = dataset.index.minute\n",
    "    dataset[\"daily_open\"] = daily_open(dataset,1)\n",
    "    dataset[\"daily_high\"] = daily_high(dataset,1)\n",
    "    dataset[\"daily_low\"] = daily_low(dataset,1)\n",
    "    dataset[\"daily_close\"] = daily_close(dataset,1)\n",
    "    print(\"Fatto\")\n",
    "    return dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a62c37a-a067-47f1-87c5-bc21c0b411e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_strategy(data,n_id):\n",
    "    #n_id=4\n",
    "    costi=data.costi.iloc[n_id]\n",
    "    direzione=data.direzione.iloc[n_id]\n",
    "    x=data.entry_rules.iloc[n_id]\n",
    "    bigpointvalue=data.bigpointvalue.iloc[n_id]\n",
    "    tick=data.tick.iloc[n_id]\n",
    "    level=data.level.iloc[n_id]\n",
    "    tipo_ordine=data.tipo_ordine.iloc[n_id]\n",
    "    exit_bars=data.exit_bars.iloc[n_id]\n",
    "    TP=data.TP.iloc[n_id]\n",
    "    SL=data.SL.iloc[n_id]\n",
    "    file_name=data.file_name.iloc[n_id]\n",
    "    uct_offset=data.uct_offset.iloc[n_id]\n",
    "    type_session=data.type_session.iloc[n_id]\n",
    "    resample_tf=data.resample_tf.iloc[n_id]\n",
    "    IS=data.start_date.iloc[n_id]\n",
    "    OOS=data.end_date.iloc[n_id]\n",
    "    custom_session_start=\"0\"+str(data.custom_session_start.iloc[n_id])\n",
    "    custom_session_stop=str(data.custom_session_stop.iloc[n_id])\n",
    "    history_temp=carica_storico_fast(file_name,uct_offset,type_session,resample_tf,IS,OOS,custom_session_start,custom_session_stop)\n",
    "    DATAFRAME_temp=history_temp.copy()\n",
    "    ADD_RULE_CONDITION=data.ADD_RULE_CONDITION.iloc[n_id]\n",
    "    backtest_strategy=strategy_numba(x,bigpointvalue,tick,costi,direzione,exit_bars,level,tipo_ordine,TP,SL,DATAFRAME_temp,ADD_RULE_CONDITION)\n",
    "    return backtest_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79341c79-9b01-4fb2-8549-7a5840d5a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "@jit(nopython=True)\n",
    "def ohlc_noise_adder_n(O, H, L, C, percentage_noise_addiction):\n",
    "\n",
    "    new_O = np.zeros(len(O))\n",
    "    new_H = np.zeros(len(H))\n",
    "    new_L = np.zeros(len(L))\n",
    "    new_C = np.zeros(len(C))\n",
    "    i = 0\n",
    "    while i < len(O):\n",
    "        factor = np.random.uniform(-1, 1)\n",
    "        single_range = H[i] - L[i]\n",
    "        new_O[i] = round(float(O[i]) + factor * (float(percentage_noise_addiction) / 100) \n",
    "                         * abs(float(single_range)), 6)\n",
    "        new_H[i] = round(float(H[i]) + factor * (float(percentage_noise_addiction) / 100) \n",
    "                             * abs(float(single_range)), 6)\n",
    "        new_L[i] = round(float(L[i]) + factor * (float(percentage_noise_addiction) / 100) \n",
    "                             * abs(float(single_range)), 6)\n",
    "        new_C[i] = round(float(C[i]) + factor * (float(percentage_noise_addiction) / 100) \n",
    "                             * abs(float(single_range)), 6)\n",
    "        app = [new_O[i],new_H[i],new_L[i],new_C[i]]\n",
    "        app.sort()\n",
    "        new_O[i] = new_O[i]\n",
    "        new_H[i] = app[-1]\n",
    "        new_L[i] = app[0]\n",
    "        new_C[i] = new_C[i]\n",
    "        i += 1\n",
    "    return new_O, new_H, new_L, new_C\n",
    "\n",
    "def Test_Noise(n_test,original_tradelist,pct_noise,pct_pass_test,stampa):\n",
    "\n",
    "    ls_noise=[]\n",
    "    for e in tqdm(range(n_test)):\n",
    "        b=ohlc_noise_adder_n(history.open.values, history.high.values, history.low.values, history.close.values, pct_noise)\n",
    "        df=history.copy()\n",
    "        df.open=b[0]\n",
    "        df.high=b[1]\n",
    "        df.low=b[2]\n",
    "        df.close=b[3]\n",
    "        trade_list_noise=strategy_numba(dna_check,bigpointvalue,tick,costi,direzione,exit_bars,level,tipo_ordine,take_p,stop_l,df,ADD_RULE_CONDITION)\n",
    "        ls_noise.append(trade_list_noise)\n",
    "    df_noise=pd.DataFrame(ls_noise).T\n",
    "    df_noise=df_noise.fillna(0)\n",
    "\n",
    "    res_ls=[]\n",
    "    for e in tqdm(range(len(df_noise.columns))):\n",
    "        tmp_strat=df_noise.iloc[:,e].to_frame()\n",
    "        tmp_strat=tmp_strat[tmp_strat!=0]\n",
    "        res_test=testNoise(original_tradelist.to_frame(),tmp_strat,pct_pass_test,stampa)\n",
    "        res_ls.append(res_test)\n",
    "        \n",
    "    return sum(res_ls)\n",
    "\n",
    "def check_best_dna(best_dnas):\n",
    "    tmp_rep=[]\n",
    "    for elm in range(len(best_dnas)):\n",
    "        tmp = fitness(best_dnas[elm])\n",
    "        tmp_rep.append(tmp)\n",
    "    return tmp_rep.index(max(tmp_rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcfddae-d25a-4206-b115-e9c5e0379126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testNoise(retestOLD,retestNoise,tol,stampa):\n",
    "    \n",
    "    pastest=[]\n",
    "    fil = [\"operation_number\",\"profit\",\"profit_factor\",\"avg_trade\",\"percent_win\",\"reward_risk_ratio\",\"maxdrawdown\",\"avgdrawdown\"]\n",
    "    ts1=pd.DataFrame()\n",
    "    for e in (retestOLD.columns.tolist()):\n",
    "        a1=operation_number(retestOLD[e])\n",
    "        a2=retestOLD[e].sum()\n",
    "        a3=profit_factor(retestOLD[e])\n",
    "        a4=avg_trade(retestOLD[e])\n",
    "        a5=percent_win(retestOLD[e])\n",
    "        a6=reward_risk_ratio(retestOLD[e])\n",
    "        a7=max_draw_down(retestOLD[e])\n",
    "        a8=avgdrawdown_nozero(retestOLD[e])\n",
    "        a9=[a1,a2,a3,a4,a5,a6,a7,a8]\n",
    "        a10=pd.DataFrame()\n",
    "        a10=pd.DataFrame([a9]).T\n",
    "        a10.columns=([e])\n",
    "        ts1=pd.concat([ts1,a10],axis=1)\n",
    "    ts1=ts1.T\n",
    "    ts1.columns=(fil)\n",
    "\n",
    "    ts2=pd.DataFrame()\n",
    "    for e in (retestNoise.columns.tolist()):\n",
    "        a1=operation_number(retestNoise[e])\n",
    "        a2=retestNoise[e].sum()\n",
    "        a3=profit_factor(retestNoise[e])\n",
    "        a4=avg_trade(retestNoise[e])\n",
    "        a5=percent_win(retestNoise[e])\n",
    "        a6=reward_risk_ratio(retestNoise[e])\n",
    "        a7=max_draw_down(retestNoise[e])\n",
    "        a8=avgdrawdown_nozero(retestNoise[e])\n",
    "        a9=[a1,a2,a3,a4,a5,a6,a7,a8]\n",
    "        a10=pd.DataFrame()\n",
    "        a10=pd.DataFrame([a9]).T\n",
    "        a10.columns=([e])\n",
    "        ts2=pd.concat([ts2,a10],axis=1)\n",
    "    ts2=ts2.T\n",
    "    ts2.columns=(fil)\n",
    "\n",
    "    for e in range (len(ts1)):\n",
    "\n",
    "        ts1_string = \"Original\"\n",
    "        ts2_string = \"Noisy\"\n",
    "\n",
    "        ts1_nop = ts1.operation_number[e]\n",
    "        ts2_nop = ts2.operation_number[e]\n",
    "        ts1_prft = ts1.profit[e]\n",
    "        ts2_prft = ts2.profit[e]\n",
    "        ts1_pf = ts1.profit_factor[e]\n",
    "        ts2_pf = ts2.profit_factor[e]\n",
    "        ts1_at = ts1.avg_trade[e]\n",
    "        ts2_at = ts2.avg_trade[e]\n",
    "        ts1_pw = ts1.percent_win[e]\n",
    "        ts2_pw = ts2.percent_win[e]\n",
    "        ts1_rrr = ts1.reward_risk_ratio[e]\n",
    "        ts2_rrr = ts2.reward_risk_ratio[e]\n",
    "        ts1_mdd = ts1.maxdrawdown[e]\n",
    "        ts2_mdd = ts2.maxdrawdown[e]\n",
    "        ts1_add = ts1.avgdrawdown[e]\n",
    "        ts2_add = ts2.avgdrawdown[e]\n",
    "\n",
    "        count=[]\n",
    "        if stampa==1:\n",
    "            print(\"\")\n",
    "            print(\"DELTA STATISTICS\",ts1.index[e],\":\")\n",
    "        \n",
    "        try:\n",
    "            if stampa==1:\n",
    "                print(\"Operations Number  :\", ts1_string, ts1_nop, ts2_string, ts2_nop, \n",
    "                      \"-> delta:\", -round((ts1_nop-ts2_nop)/ts1_nop*100), \"%\")\n",
    "            x=round((ts1_nop-ts2_nop)/ts1_nop*100)\n",
    "            if (x>=-tol) & (x<=tol):\n",
    "                count.append(1)\n",
    "            if stampa==1:\n",
    "                print(\"Profit             :\", ts1_string, round(ts1_prft,2), ts2_string, round(ts2_prft,2), \n",
    "                      \"-> delta:\", -round((ts1_prft-ts2_prft)/ts1_prft*100), \"%\")\n",
    "            x=round((ts1_prft-ts2_prft)/ts1_prft*100)\n",
    "            if (x>=-tol) & (x<=tol):\n",
    "                count.append(1)\n",
    "            if stampa==1:\n",
    "                print(\"Profit Factor      :\", ts1_string, round(ts1_pf,2), ts2_string, round(ts2_pf,2), \n",
    "                      \"-> delta:\", -round((ts1_pf-ts2_pf)/ts1_pf*100), \"%\")\n",
    "            x=round((ts1_pf-ts2_pf)/ts1_pf*100)\n",
    "            if (x>=-tol) & (x<=tol):\n",
    "                count.append(1)\n",
    "            if stampa==1:\n",
    "                print(\"Average Trade      :\", ts1_string, round(ts1_at,2), ts2_string, round(ts2_at,2), \n",
    "                      \"-> delta:\", -round((ts1_at-ts2_at)/ts1_at*100), \"%\")\n",
    "            x=round((ts1_at-ts2_at)/ts1_at*100)\n",
    "            if (x>=-tol) & (x<=tol):\n",
    "                count.append(1)\n",
    "            if stampa==1:\n",
    "                print(\"Percent Win        :\", ts1_string, round(ts1_pw,2), \"%\", ts2_string, round(ts2_pw,2),\"%\", \n",
    "                      \"-> delta:\", -round((ts1_pw-ts2_pw)/ts1_pw*100), \"%\")\n",
    "            x=round((ts1_pw-ts2_pw)/ts1_pw*100)\n",
    "            if (x>=-tol) & (x<=tol):\n",
    "                count.append(1)\n",
    "            if stampa==1:\n",
    "                print(\"Reward Risk Ratio  :\", ts1_string, round(ts1_rrr,2), ts2_string, round(ts2_rrr,2), \n",
    "                      \"-> delta:\", -round((ts1_rrr-ts2_rrr)/ts1_rrr*100), \"%\")\n",
    "            x=round((ts1_rrr-ts2_rrr)/ts1_rrr*100)\n",
    "            if (x>=-tol) & (x<=tol):\n",
    "                count.append(1)\n",
    "\n",
    "            if stampa==1:\n",
    "                print(\"Max Draw Down      :\", ts1_string, round(ts1_mdd,2), ts2_string, round(ts2_mdd,2), \n",
    "                      \"-> delta:\", -round((ts1_mdd-ts2_mdd)/ts1_mdd*100), \"%\")\n",
    "            x=round((ts1_mdd-ts2_mdd)/ts1_mdd*100)\n",
    "            if (x>=-tol) & (x<=tol):\n",
    "                count.append(1)\n",
    "                \n",
    "            if stampa==1:\n",
    "                print(\"Avg Draw Down      :\", ts1_string, round(ts1_add,2), ts2_string, round(ts2_add,2), \n",
    "                      \"-> delta:\", -round((ts1_add-ts2_add)/ts1_add*100), \"%\")\n",
    "            x=round((ts1_add-ts2_add)/ts1_add*100)\n",
    "            if (x>=-tol) & (x<=tol):\n",
    "                count.append(1)\n",
    "\n",
    "            if sum(count) == 8:\n",
    "                pastest.append(str(ts1.index[e]))\n",
    "        except OverflowError:\n",
    "            continue\n",
    "        except ValueError:\n",
    "            continue\n",
    "        if stampa==1:\n",
    "            print(\"\")\n",
    "            print(\"Passano\",sum(count),\"metriche su 8\")\n",
    "    return sum(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a62118-0368-4be6-8b68-a7227c61cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ranking(split_dataframe,ranking,soglia_minima):\n",
    "\n",
    "    ls_=[]\n",
    "    for e in split_dataframe:\n",
    "        #print(eval(ranking)(e))#75\n",
    "        ls_.append(eval(ranking)(e))\n",
    "        media=sum(ls_)/len(ls_)\n",
    "    #print(media)  \n",
    "\n",
    "    results_avgt=[]\n",
    "    for E in ls_:\n",
    "        results_avgt.append((E<(media*1.5)) & (E>(media*0.5)) & (media>soglia_minima))    \n",
    "\n",
    "    if sum(results_avgt)==len(ls_):\n",
    "        #print(\"OK\")\n",
    "        return 1\n",
    "    else:\n",
    "        #print(\"KO\")\n",
    "        return 0\n",
    "    \n",
    "def ret_DD(dataframe):\n",
    "    x=dataframe.sum()/abs(max_draw_down(dataframe.cumsum()))\n",
    "    return x\n",
    "\n",
    "def check_days(split_dataframe):\n",
    "    #print((split_dataframe.index[-1]-split_dataframe.index[0]).days)\n",
    "    return (split_dataframe.index[-1]-split_dataframe.index[0]).days\n",
    "\n",
    "def Split_Validation(dataframe,n_split):\n",
    "    strategy_agg=dataframe.sum(axis=1).resample(\"D\").sum()\n",
    "    strategy_agg=strategy_agg[strategy_agg!=0].dropna()\n",
    "    #strategy_agg.cumsum().plot()\n",
    "    split_dataframe=np.array_split(strategy_agg,n_split)\n",
    "    \"\"\"for e in range(len(split_dataframe)-1):\n",
    "        plt.axvline(x=split_dataframe[e].index[-1])\n",
    "    plt.show()\"\"\"\n",
    "    check1=check_ranking(split_dataframe,\"ret_DD\",1.2)\n",
    "    check2=check_ranking(split_dataframe,\"avg_trade\",75)\n",
    "    check3=check_ranking(split_dataframe,\"profit_factor\",1.25)\n",
    "    #check4=check_ranking(split_dataframe,\"check_days\",check_days(strategy_agg)//n_split)\n",
    "    if check1+check2+check3>=2:\n",
    "        print(\"STRATEGIA OK\")\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "644a1f8e-59e3-46a2-984d-0182bf188a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_translator():    \n",
    "    \n",
    "    \n",
    "\n",
    "    pattern_dict = {}\n",
    "    \n",
    "    pattern_dict['p1'] = ['myBodyD1 = round(abs(dataset.daily_open - dataset.daily_close),2)',\n",
    "                          'myRangeD1 = dataset.daily_high - dataset.daily_low',\n",
    "                          'dataset[\"p1\"] = myBodyD1<(0.1*myRangeD1)']\n",
    "                          \n",
    "    pattern_dict['p2'] = ['myBodyD1 = round(abs(dataset.daily_open - dataset.daily_close),2)',\n",
    "                          'myRangeD1 = dataset.daily_high - dataset.daily_low',\n",
    "                          'dataset[\"p2\"] = myBodyD1<(0.25*myRangeD1)']  \n",
    "    \n",
    "    pattern_dict['p3'] = ['myBodyD1 = round(abs(dataset.daily_open - dataset.daily_close),2)',\n",
    "                          'myRangeD1 = dataset.daily_high - dataset.daily_low',\n",
    "                          'dataset[\"p3\"]= myBodyD1<(0.5*myRangeD1)']\n",
    "    \n",
    "    pattern_dict['p4'] = ['myBodyD1 = round(abs(dataset.daily_open - dataset.daily_close),2)',\n",
    "                          'myRangeD1 = dataset.daily_high - dataset.daily_low',\n",
    "                          'dataset[\"p4\"] = myBodyD1<(0.75*myRangeD1)'] \n",
    "    \n",
    "    pattern_dict['p5'] = ['myBodyD1 = round(abs(dataset.daily_open - dataset.daily_close),2)',\n",
    "                          'myRangeD1 = dataset.daily_high - dataset.daily_low',\n",
    "                          'dataset[\"p5\"] = myBodyD1>(0.25*myRangeD1)'] \n",
    "\n",
    "    pattern_dict['p6'] = ['myBodyD1 = round(abs(dataset.daily_open - dataset.daily_close),2)',\n",
    "                          'myRangeD1 = dataset.daily_high - dataset.daily_low',\n",
    "                          'dataset[\"p6\"] = myBodyD1>(0.5*myRangeD1)'] \n",
    "    \n",
    "    pattern_dict['p7'] = ['myBodyD1 = round(abs(dataset.daily_open - dataset.daily_close),2)',\n",
    "                          'myRangeD1 = dataset.daily_high - dataset.daily_low',\n",
    "                          'dataset[\"p7\"]= myBodyD1>(0.75*myRangeD1)'] \n",
    "    \n",
    "    pattern_dict['p8'] = ['myBodyD1 = round(abs(dataset.daily_open - dataset.daily_close),2)',\n",
    "                          'myRangeD1 = dataset.daily_high - dataset.daily_low',\n",
    "                          'dataset[\"p8\"] = myBodyD1>(1*myRangeD1)'] \n",
    "    \n",
    "    pattern_dict['p9'] = ['myBodyD1 = round(abs(dataset.daily_open - dataset.daily_close),2)',\n",
    "                          'daily_high5 = daily_high(dataset,5)',\n",
    "                          'daily_low5 = daily_low(dataset,5)',\n",
    "                          'myRangeD5 = daily_high5 - daily_low5',\n",
    "                          'dataset[\"p9\"] = myBodyD1<(0.5*myRangeD5)']\n",
    "    \n",
    "    pattern_dict['p10'] = ['myBodyD1 = round(abs(dataset.daily_open - dataset.daily_close),2)',\n",
    "                           'daily_high2 = daily_high(dataset,2)',\n",
    "                           'daily_high3 = daily_high(dataset,3)',\n",
    "                           'daily_high4 = daily_high(dataset,4)',\n",
    "                           'daily_high5 = daily_high(dataset,5)',\n",
    "                           'daily_high6 = daily_high(dataset,6)',\n",
    "                           'daily_high7 = daily_high(dataset,7)',\n",
    "                           'daily_high8 = daily_high(dataset,8)',\n",
    "                           'daily_high9 = daily_high(dataset,9)',\n",
    "                           'daily_high0 = daily_high(dataset,10)',\n",
    "                           'daily_low2 = daily_low(dataset,2)',\n",
    "                           'daily_low3 = daily_low(dataset,3)',\n",
    "                           'daily_low4 = daily_low(dataset,4)',\n",
    "                           'daily_low5 = daily_low(dataset,5)',\n",
    "                           'daily_low6 = daily_low(dataset,5)',\n",
    "                           'daily_low7 = daily_low(dataset,7)',\n",
    "                           'daily_low8 = daily_low(dataset,8)',\n",
    "                           'daily_low9 = daily_low(dataset,9)',\n",
    "                           'daily_low0 = daily_low(dataset,10)',  \n",
    "                           'myRangeMaxD10 = (highest_serie(dataset.daily_high,daily_high2,daily_high3,daily_high4,daily_high5,daily_high6,daily_high7,daily_high8,daily_high9,daily_high0) -\\\n",
    "                           (lowest_serie(dataset.daily_low,daily_low2,daily_low3,daily_low4,daily_low5,daily_low6,daily_low7,daily_low8,daily_low9,daily_low0)))',\n",
    "                           'dataset[\"p10\"] = myBodyD1<(0.5*myRangeMaxD10)']\n",
    "    \n",
    "    pattern_dict['p11'] = ['daily_high5 = daily_high(dataset,5)',\n",
    "                          'daily_open5 = daily_open(dataset,5)',\n",
    "                          'myBodyD5 = round(abs(daily_open5 - dataset.daily_close),2)',\n",
    "                          'dataset[\"p11\"] = myBodyD5 < (0.1*(daily_high5 - dataset.daily_low))']\n",
    "    \n",
    "    pattern_dict['p12'] = ['daily_high5 = daily_high(dataset,5)',\n",
    "                          'daily_open5 = daily_open(dataset,5)',\n",
    "                          'myBodyD5 = round(abs(daily_open5 - dataset.daily_close),2)',\n",
    "                          'dataset[\"p12\"] = myBodyD5 < (0.25*(daily_high5-dataset.daily_low))']\n",
    "    \n",
    "    pattern_dict['p13'] = ['daily_high5 = daily_high(dataset,5)',\n",
    "                          'daily_open5 = daily_open(dataset,5)',\n",
    "                          'myBodyD5 = round(abs(daily_open5 - dataset.daily_close),2)',\n",
    "                          'dataset[\"p13\"] = myBodyD5 < (0.50*(daily_high5-dataset.daily_low))']\n",
    "    \n",
    "    pattern_dict['p14'] = ['daily_high5 = daily_high(dataset,5)',\n",
    "                          'daily_open5 = daily_open(dataset,5)',\n",
    "                          'myBodyD5 = round(abs(daily_open5 - dataset.daily_close),2)',\n",
    "                          'dataset[\"p14\"] = myBodyD5 < (0.75*(daily_high5-dataset.daily_low))']\n",
    "    \n",
    "    pattern_dict['p15'] = ['daily_high5 = daily_high(dataset,5)',\n",
    "                          'daily_open5 = daily_open(dataset,5)',\n",
    "                          'myBodyD5 = round(abs(daily_open5 - dataset.daily_close),2)',\n",
    "                          'dataset[\"p15\"] = myBodyD5 < (1*(daily_high5-dataset.daily_low))']\n",
    "    \n",
    "    pattern_dict['p16'] = ['daily_high5 = daily_high(dataset,5)',\n",
    "                          'daily_open5 = daily_open(dataset,5)',\n",
    "                          'myBodyD5 = round(abs(daily_open5 - dataset.daily_close),2)',\n",
    "                          'dataset[\"p16\"] = myBodyD5 < (1.5*(daily_high5-dataset.daily_low))']\n",
    "    \n",
    "    pattern_dict['p17'] = ['daily_high5 = daily_high(dataset,5)',\n",
    "                          'daily_open5 = daily_open(dataset,5)',\n",
    "                          'myBodyD5 = round(abs(daily_open5 - dataset.daily_close),2)',\n",
    "                          'dataset[\"p17\"] = myBodyD5 < (2*(daily_high5-dataset.daily_low))']\n",
    "    \n",
    "    pattern_dict['p18'] = ['daily_high5 = daily_high(dataset,5)',\n",
    "                          'daily_open5 = daily_open(dataset,5)',\n",
    "                          'myBodyD5 = round(abs(daily_open5 - dataset.daily_close),2)',\n",
    "                          'dataset[\"p18\"] = myBodyD5 > (0.25*(daily_high5-dataset.daily_low))']\n",
    "    \n",
    "    pattern_dict['p19'] = ['daily_high5 = daily_high(dataset,5)',\n",
    "                          'daily_open5 = daily_open(dataset,5)',\n",
    "                          'myBodyD5 = round(abs(daily_open5 - dataset.daily_close),2)',\n",
    "                          'dataset[\"p19\"] = myBodyD5 > (0.50*(daily_high5-dataset.daily_low))']\n",
    "    \n",
    "    pattern_dict['p20'] = ['daily_high5 = daily_high(dataset,5)',\n",
    "                          'daily_open5 = daily_open(dataset,5)',\n",
    "                          'myBodyD5 = round(abs(daily_open5 - dataset.daily_close),2)',\n",
    "                          'dataset[\"p20\"] = myBodyD5 > (0.75*(daily_high5-dataset.daily_low))']\n",
    "    \n",
    "    pattern_dict['p21'] = ['daily_high5 = daily_high(dataset,5)',\n",
    "                          'daily_open5 = daily_open(dataset,5)',\n",
    "                          'myBodyD5 = round(abs(daily_open5 - dataset.daily_close),2)',\n",
    "                          'dataset[\"p21\"] = myBodyD5 > (1*(daily_high5-dataset.daily_low))']\n",
    "    \n",
    "    pattern_dict['p22'] = ['daily_high5 = daily_high(dataset,5)',\n",
    "                          'daily_open5 = daily_open(dataset,5)',\n",
    "                          'myBodyD5 = round(abs(daily_open5 - dataset.daily_close),2)',\n",
    "                          'dataset[\"p22\"] = myBodyD5 > (1.5*(daily_high5-dataset.daily_low))']\n",
    "    \n",
    "    pattern_dict['p23'] = ['daily_high5 = daily_high(dataset,5)',\n",
    "                          'daily_open5 = daily_open(dataset,5)',\n",
    "                          'myBodyD5 = round(abs(daily_open5 - dataset.daily_close),2)',\n",
    "                          'dataset[\"p23\"] = myBodyD5 > (2*(daily_high5-dataset.daily_low))']\n",
    "    \n",
    "    pattern_dict['p24'] = ['daily_high5 = daily_high(dataset,5)',\n",
    "                          'daily_open5 = daily_open(dataset,5)',\n",
    "                          'myBodyD5 = round(abs(daily_open5 - dataset.daily_close),2)',\n",
    "                          'dataset[\"p24\"] = myBodyD5 > (2.5*(daily_high5-dataset.daily_low))']\n",
    "    \n",
    "    pattern_dict['p25'] = ['daily_high5 = daily_high(dataset,5)',\n",
    "                           'daily_low5 = daily_low(dataset,5)',\n",
    "                           'daily_open5 = daily_open(dataset,5)',\n",
    "                           'myBodyD5 = round(abs(daily_open5 - dataset.daily_close),2)',\n",
    "                           'myRangeD5 = daily_high5 - daily_low5',                           \n",
    "                           'dataset[\"p25\"] = myBodyD5 < (0.1*myRangeD5)']\n",
    "    \n",
    "    pattern_dict['p26'] = ['daily_high5 = daily_high(dataset,5)',\n",
    "                           'daily_low5 = daily_low(dataset,5)',\n",
    "                           'daily_open5 = daily_open(dataset,5)',\n",
    "                           'myBodyD5 = round(abs(daily_open5 - dataset.daily_close),2)',\n",
    "                           'myRangeD5 = daily_high5 - daily_low5',                           \n",
    "                           'dataset[\"p26\"] = myBodyD5 < (0.25*myRangeD5)']\n",
    "    \n",
    "    pattern_dict['p27'] = ['daily_high5 = daily_high(dataset,5)',\n",
    "                           'daily_low5 = daily_low(dataset,5)',\n",
    "                           'daily_open5 = daily_open(dataset,5)',\n",
    "                           'myBodyD5 = round(abs(daily_open5 - dataset.daily_close),2)',\n",
    "                           'myRangeD5 = daily_high5 - daily_low5',                           \n",
    "                           'dataset[\"p27\"] = myBodyD5 < (0.5*myRangeD5)']\n",
    "    \n",
    "    pattern_dict['p28'] = ['daily_high5 = daily_high(dataset,5)',\n",
    "                           'daily_low5 = daily_low(dataset,5)',\n",
    "                           'daily_open5 = daily_open(dataset,5)',\n",
    "                           'myBodyD5 = round(abs(daily_open5 - dataset.daily_close),2)',\n",
    "                           'myRangeD5 = daily_high5 - daily_low5',                           \n",
    "                           'dataset[\"p28\"] = myBodyD5 < (0.75*myRangeD5)']\n",
    "    \n",
    "    pattern_dict['p29'] = ['daily_high5 = daily_high(dataset,5)',\n",
    "                           'daily_low5 = daily_low(dataset,5)',\n",
    "                           'daily_open5 = daily_open(dataset,5)',\n",
    "                           'myBodyD5 = round(abs(daily_open5 - dataset.daily_close),2)',\n",
    "                           'myRangeD5 = daily_high5 - daily_low5',                           \n",
    "                           'dataset[\"p29\"] = myBodyD5 > (0.1*myRangeD5)']\n",
    "    \n",
    "    pattern_dict['p30'] = ['daily_high5 = daily_high(dataset,5)',\n",
    "                           'daily_low5 = daily_low(dataset,5)',\n",
    "                           'daily_open5 = daily_open(dataset,5)',\n",
    "                           'myBodyD5 = round(abs(daily_open5 - dataset.daily_close),2)',\n",
    "                           'myRangeD5 = daily_high5 - daily_low5',                           \n",
    "                           'dataset[\"p30\"] = myBodyD5 > (0.25*myRangeD5)']\n",
    "    \n",
    "    pattern_dict['p31'] = ['daily_high5 = daily_high(dataset,5)',\n",
    "                           'daily_low5 = daily_low(dataset,5)',\n",
    "                           'daily_open5 = daily_open(dataset,5)',\n",
    "                           'myBodyD5 = round(abs(daily_open5 - dataset.daily_close),2)',\n",
    "                           'myRangeD5 = daily_high5 - daily_low5',                           \n",
    "                           'dataset[\"p31\"] = myBodyD5 > (0.5*myRangeD5)']\n",
    "    \n",
    "    pattern_dict['p32'] = ['daily_high5 = daily_high(dataset,5)',\n",
    "                           'daily_low5 = daily_low(dataset,5)',\n",
    "                           'daily_open5 = daily_open(dataset,5)',\n",
    "                           'myBodyD5 = round(abs(daily_open5 - dataset.daily_close),2)',\n",
    "                           'myRangeD5 = daily_high5 - daily_low5',                           \n",
    "                           'dataset[\"p32\"] = myBodyD5 > (0.75*myRangeD5)']\n",
    "    \n",
    "    pattern_dict['p33'] = ['daily_close2 = daily_close(dataset,2)',\n",
    "                           'daily_close3 = daily_close(dataset,3)',\n",
    "                           'daily_close4 = daily_close(dataset,4)',\n",
    "                           'dataset[\"p33\"] = (dataset.daily_close>daily_close2) & (daily_close2>daily_close3) & (daily_close3>daily_close4)']\n",
    "    \n",
    "    pattern_dict['p34'] = ['daily_close2 = daily_close(dataset,2)',\n",
    "                           'daily_close3 = daily_close(dataset,3)',\n",
    "                           'daily_close4 = daily_close(dataset,4)',\n",
    "                           'dataset[\"p34\"] = (dataset.daily_close<daily_close2) & (daily_close2<daily_close3) & (daily_close3<daily_close4)']\n",
    "    \n",
    "    pattern_dict['p35'] = ['daily_close2 = daily_close(dataset,2)',\n",
    "                           'daily_close3 = daily_close(dataset,3)',\n",
    "                           'daily_close4 = daily_close(dataset,4)',\n",
    "                           'daily_close5 = daily_close(dataset,5)',\n",
    "                           'dataset[\"p35\"] = (dataset.daily_close>daily_close2) & (daily_close2>daily_close3) & (daily_close3>daily_close4) & (daily_close4>daily_close5)']\n",
    "    \n",
    "    pattern_dict['p36'] = ['daily_close2 = daily_close(dataset,2)',\n",
    "                           'daily_close3 = daily_close(dataset,3)',\n",
    "                           'daily_close4 = daily_close(dataset,4)',\n",
    "                           'daily_close5 = daily_close(dataset,5)',\n",
    "                           'dataset[\"p35\"] = dataset[\"p36\"] = (dataset.daily_close<daily_close2) & (daily_close2<daily_close3) & (daily_close3<daily_close4) & (daily_close4<daily_close5)']\n",
    "    \n",
    "    pattern_dict['p37'] = ['daily_high2 = daily_high(dataset,2)',\n",
    "                           'daily_low2 = daily_low(dataset,2)',  \n",
    "                           'dataset[\"p37\"] = (dataset.daily_high>daily_high2) & (dataset.daily_low>daily_low2)']\n",
    "    \n",
    "    pattern_dict['p38'] = ['daily_high2 = daily_high(dataset,2)',\n",
    "                           'daily_low2 = daily_low(dataset,2)',  \n",
    "                           'dataset[\"p38\"] = (dataset.daily_high<daily_high2) & (dataset.daily_low<daily_low2)']\n",
    "    \n",
    "    pattern_dict['p39'] = ['daily_close2 = daily_close(dataset,2)',                             \n",
    "                           'dataset[\"p39\"] = (dataset.daily_close>daily_close2)']\n",
    "    \n",
    "    pattern_dict['p40'] = ['daily_close2 = daily_close(dataset,2)',                             \n",
    "                           'dataset[\"p40\"] = (dataset.daily_close<daily_close2)']\n",
    "    \n",
    "    pattern_dict['p41'] = ['dataset[\"p41\"] = (dataset.daily_close<dataset.daily_open)']\n",
    "    \n",
    "    pattern_dict['p42'] = ['dataset[\"p42\"] = (dataset.daily_close>dataset.daily_open)']\n",
    "    \n",
    "    pattern_dict['p43'] = ['daily_close2 = daily_close(dataset,2)', \n",
    "                           'dataset[\"p43\"] = (dataset.daily_close<(daily_close2-(daily_close2*0.5*0.01)))']\n",
    "    \n",
    "    pattern_dict['p44'] = ['daily_close2 = daily_close(dataset,2)', \n",
    "                           'dataset[\"p44\"] = (dataset.daily_close<(daily_close2-(daily_close2*1*0.01)))']\n",
    "    \n",
    "    pattern_dict['p45'] = ['daily_close2 = daily_close(dataset,2)', \n",
    "                           'dataset[\"p45\"] = (dataset.daily_close<(daily_close2-(daily_close2* 1.5*0.01)))']\n",
    "    \n",
    "    pattern_dict['p46'] = ['daily_close2 = daily_close(dataset,2)', \n",
    "                           'dataset[\"p46\"] = (dataset.daily_close<(daily_close2-(daily_close2*2*0.01)))']\n",
    "    \n",
    "    pattern_dict['p47'] = ['daily_close2 = daily_close(dataset,2)', \n",
    "                           'dataset[\"p47\"] = (dataset.daily_close<(daily_close2-(daily_close2*2.5*0.01)))']\n",
    "    \n",
    "    pattern_dict['p48'] = ['daily_close2 = daily_close(dataset,2)', \n",
    "                           'dataset[\"p48\"] = (dataset.daily_close<(daily_close2-(daily_close2*3*0.01)))']\n",
    "    \n",
    "    pattern_dict['p49'] = ['daily_close2 = daily_close(dataset,2)', \n",
    "                           'dataset[\"p49\"] = (dataset.daily_close>(daily_close2+(daily_close2*0.5*0.01)))']\n",
    "    \n",
    "    pattern_dict['p50'] = ['daily_close2 = daily_close(dataset,2)', \n",
    "                           'dataset[\"p50\"] = (dataset.daily_close>(daily_close2+(daily_close2*1*0.01)))']\n",
    "    \n",
    "    pattern_dict['p51'] = ['daily_close2 = daily_close(dataset,2)', \n",
    "                           'dataset[\"p51\"] = (dataset.daily_close>(daily_close2+(daily_close2*1.5*0.01)))']\n",
    "    \n",
    "    pattern_dict['p52'] = ['daily_close2 = daily_close(dataset,2)', \n",
    "                           'dataset[\"p52\"] = (dataset.daily_close>(daily_close2+(daily_close2*2*0.01)))']\n",
    "    \n",
    "    pattern_dict['p53'] = ['daily_close2 = daily_close(dataset,2)', \n",
    "                           'dataset[\"p53\"] = (dataset.daily_close>(daily_close2+(daily_close2*2.5*0.01)))']\n",
    "    \n",
    "    pattern_dict['p54'] = ['daily_close2 = daily_close(dataset,2)', \n",
    "                           'dataset[\"p54\"] = (dataset.daily_close>(daily_close2+(daily_close2*3*0.01)))']\n",
    "    \n",
    "    pattern_dict['p55'] = ['daily_high5 = daily_high(dataset,5)', \n",
    "                           'dataset[\"p55\"] = dataset.daily_high>daily_high5']\n",
    "    \n",
    "    pattern_dict['p56'] = ['daily_high5 = daily_high(dataset,5)', \n",
    "                           'dataset[\"p56\"] = dataset.daily_high<daily_high5']\n",
    "    \n",
    "    pattern_dict['p57'] = ['daily_low5 = daily_low(dataset,5)', \n",
    "                           'dataset[\"p57\"] = dataset.daily_low>daily_low5']\n",
    "    \n",
    "    pattern_dict['p58'] = ['daily_low5 = daily_low(dataset,5)', \n",
    "                           'dataset[\"p58\"] = dataset.daily_low<daily_low5']\n",
    "    \n",
    "    pattern_dict['p59'] = ['daily_high2 = daily_high(dataset,2)',\n",
    "                           'daily_high3 = daily_high(dataset,3)',\n",
    "                           'daily_high4 = daily_high(dataset,4)',                           \n",
    "                           'dataset[\"p59\"] = (dataset.daily_high>daily_high2)&(dataset.daily_high>daily_high3)&(dataset.daily_high>daily_high4)']\n",
    "    \n",
    "    pattern_dict['p59'] = ['daily_high2 = daily_high(dataset,2)',\n",
    "                           'daily_high3 = daily_high(dataset,3)',\n",
    "                           'daily_high4 = daily_high(dataset,4)',                           \n",
    "                           'dataset[\"p59\"] = (dataset.daily_high>daily_high2)&(dataset.daily_high>daily_high3)&(dataset.daily_high>daily_high4)']\n",
    "    \n",
    "    pattern_dict['p60'] = ['daily_high2 = daily_high(dataset,2)',\n",
    "                           'daily_high3 = daily_high(dataset,3)',\n",
    "                           'daily_high4 = daily_high(dataset,4)',                           \n",
    "                           'dataset[\"p60\"] = (dataset.daily_high<daily_high2)&(dataset.daily_high<daily_high3)&(dataset.daily_high<daily_high4)']\n",
    "    \n",
    "    pattern_dict['p61'] = ['daily_low2 = daily_low(dataset,2)',\n",
    "                           'daily_low3 = daily_low(dataset,3)',\n",
    "                           'daily_low4 = daily_low(dataset,4)',                           \n",
    "                           'dataset[\"p61\"] = (dataset.daily_low<daily_low2)&(dataset.daily_low<daily_low3)&(dataset.daily_low<daily_low4)']\n",
    "    \n",
    "    pattern_dict['p62'] = ['daily_low2 = daily_low(dataset,2)',\n",
    "                           'daily_low3 = daily_low(dataset,3)',\n",
    "                           'daily_low4 = daily_low(dataset,4)',                           \n",
    "                           'dataset[\"p62\"] = (dataset.daily_low>daily_low2)&(dataset.daily_low>daily_low3)&(dataset.daily_low>daily_low4)']\n",
    "    \n",
    "    pattern_dict['p63'] = ['daily_close2 = daily_close(dataset,2)',\n",
    "                           'daily_close3 = daily_close(dataset,3)',                                                \n",
    "                           'dataset[\"p63\"] = (dataset.daily_close>daily_close2)&(dataset.daily_close>daily_close3)&(dataset.daily_close>dataset.daily_open)']\n",
    "    \n",
    "    pattern_dict['p64'] = ['daily_close2 = daily_close(dataset,2)',\n",
    "                           'daily_close3 = daily_close(dataset,3)',                                                \n",
    "                           'dataset[\"p64\"] = (dataset.daily_close<daily_close2)&(dataset.daily_close<daily_close3)&(dataset.daily_close<dataset.daily_open)']\n",
    "    \n",
    "    pattern_dict['p65'] = ['myRangeD1 = dataset.daily_high - dataset.daily_low',                                              \n",
    "                           'dataset[\"p65\"] = (dataset.daily_high-dataset.daily_close)<0.20*myRangeD1']\n",
    "    \n",
    "    pattern_dict['p66'] = ['myRangeD1 = dataset.daily_high - dataset.daily_low',                                              \n",
    "                           'dataset[\"p66\"] = (dataset.daily_high-dataset.daily_low)<0.20*myRangeD1']\n",
    "    \n",
    "    pattern_dict['p67'] = ['myRangeD1 = dataset.daily_high - dataset.daily_low',\n",
    "                           'daily_high2 = daily_high(dataset,2)',\n",
    "                           'daily_low2 = daily_low(dataset,2)',                           \n",
    "                           'dataset[\"p67\"] = (myRangeD1<(((daily_high2-daily_low2)+(daily_high2-daily_low2))/3))']\n",
    "    \n",
    "    pattern_dict['p68'] = ['myRangeD1 = dataset.daily_high - dataset.daily_low',\n",
    "                           'daily_high2 = daily_high(dataset,2)',\n",
    "                           'daily_high3 = daily_high(dataset,3)',\n",
    "                           'daily_low2 = daily_low(dataset,2)',\n",
    "                           'daily_low3 = daily_low(dataset,3)',\n",
    "                           'myRangeD2 = daily_high2 - daily_low2',\n",
    "                           'myRangeD3 = daily_high3 - daily_low3',\n",
    "                           'dataset[\"p68\"] = (myRangeD1<myRangeD2)&(myRangeD2<myRangeD3)']\n",
    "    \n",
    "    pattern_dict['p69'] = ['daily_high2 = daily_high(dataset,2)',\n",
    "                           'daily_low2 = daily_low(dataset,2)',                           \n",
    "                           'dataset[\"p69\"] = (daily_high2>dataset.daily_high)&(daily_low2 < dataset.daily_low)']\n",
    "    \n",
    "    pattern_dict['p70'] = ['daily_high2 = daily_high(dataset,2)',\n",
    "                           'dataset[\"p70\"] = (dataset.daily_high<daily_high2)']\n",
    "    \n",
    "    pattern_dict['p71'] = ['daily_low2 = daily_low(dataset,2)',\n",
    "                           'dataset[\"p71\"] = (dataset.daily_low>daily_low2)']\n",
    "    \n",
    "    pattern_dict['p72'] = ['daily_low2 = daily_low(dataset,2)',\n",
    "                           'dataset[\"p72\"] = (dataset.daily_low<daily_low2)']\n",
    "    \n",
    "    pattern_dict['p73'] = ['daily_high2 = daily_high(dataset,2)',\n",
    "                           'daily_low2 = daily_low(dataset,2)',                            \n",
    "                           'dataset[\"p73\"] = (daily_high2>dataset.daily_high)|(daily_low2<dataset.daily_low)']\n",
    "    \n",
    "    pattern_dict['p74'] = ['daily_high2 = daily_high(dataset,2)',\n",
    "                           'daily_low2 = daily_low(dataset,2)',                            \n",
    "                           'dataset[\"p74\"] = (daily_high2<dataset.daily_high)&(daily_low2>dataset.daily_low)']\n",
    "    \n",
    "    pattern_dict['p75'] = ['daily_close2 = daily_close(dataset,2)',\n",
    "                           'daily_open2 = daily_open(dataset,2)',                            \n",
    "                           'dataset[\"p75\"] = (dataset.daily_close>dataset.daily_open) & (daily_close2>daily_open2)']\n",
    "    \n",
    "    pattern_dict['p76'] = ['daily_close2 = daily_close(dataset,2)',\n",
    "                           'daily_open2 = daily_open(dataset,2)',                            \n",
    "                           'dataset[\"p76\"] = (dataset.daily_close<dataset.daily_open) & (daily_close2>daily_open2)']\n",
    "    \n",
    "    pattern_dict['p77'] = ['daily_close2 = daily_close(dataset,2)',\n",
    "                           'daily_open2 = daily_open(dataset,2)',                            \n",
    "                           'dataset[\"p77\"] = (dataset.daily_close<dataset.daily_open) & (daily_close2<daily_open2)']\n",
    "    \n",
    "    pattern_dict['p78'] = ['daily_open2 = daily_open(dataset,2)',\n",
    "                           'daily_high2 = daily_high(dataset,2)',\n",
    "                           'daily_low2 = daily_low(dataset,2)',                           \n",
    "                           'myMaxHighD2 = highest_serie(dataset.daily_high,daily_high2)',\n",
    "                           'myMinLowD2 = lowest_serie(dataset.daily_low,daily_low2)',  \n",
    "                           'dataset[\"p78\"] = abs(daily_open2-dataset.daily_close) < (0.5*(myMaxHighD2 - myMinLowD2))']\n",
    "    \n",
    "    pattern_dict['p79'] = ['daily_high2 = daily_high(dataset,2)',                          \n",
    "                           'dataset[\"p79\"] = (dataset.daily_high> daily_high2)']\n",
    "    \n",
    "    pattern_dict['p80'] = ['daily_high2 = daily_high(dataset,2)',   \n",
    "                           'daily_high3 = daily_high(dataset,3)',\n",
    "                           'dataset[\"p80\"] = (dataset.daily_high> daily_high2) & (dataset.daily_high>daily_high3)']\n",
    "    \n",
    "    pattern_dict['p81'] = ['daily_high2 = daily_high(dataset,2)',   \n",
    "                           'daily_high3 = daily_high(dataset,3)',\n",
    "                           'dataset[\"p81\"] = (dataset.daily_high< daily_high2) & (dataset.daily_high< daily_high3)']\n",
    "    \n",
    "    pattern_dict['p82'] = ['daily_low2 = daily_low(dataset,2)',                          \n",
    "                           'dataset[\"p82\"] = (dataset.daily_low<daily_low2)']\n",
    "    \n",
    "    pattern_dict['p82'] = ['daily_low2 = daily_low(dataset,2)', \n",
    "                           'daily_low3 = daily_low(dataset,2)',                            \n",
    "                           'dataset[\"p82\"] = (dataset.daily_low>daily_low2) & (dataset.daily_low>daily_low3)']\n",
    "    \n",
    "    pattern_dict['p83'] = ['daily_low2 = daily_low(dataset,2)', \n",
    "                           'daily_low3 = daily_low(dataset,2)',                            \n",
    "                           'dataset[\"p83\"] = (dataset.daily_low<daily_low2) & (dataset.daily_low<daily_low3)']\n",
    "    \n",
    "    pattern_dict['p84'] = ['daily_low2 = daily_low(dataset,2)', \n",
    "                           'daily_low3 = daily_low(dataset,2)', \n",
    "                           'daily_high2 = daily_high(dataset,2)', \n",
    "                           'daily_high3 = daily_high(dataset,3)', \n",
    "                           'dataset[\"p84\"] = (dataset.daily_high>daily_high2) & (dataset.daily_low>daily_low2) & (daily_high2<daily_high3) & (daily_low2<daily_low3)']\n",
    "    \n",
    "    pattern_dict['p85'] = ['daily_low2 = daily_low(dataset,2)', \n",
    "                           'daily_low3 = daily_low(dataset,2)', \n",
    "                           'daily_high2 = daily_high(dataset,2)', \n",
    "                           'daily_high3 = daily_high(dataset,3)', \n",
    "                           'dataset[\"p85\"] = (dataset.daily_high<daily_high2) & (dataset.daily_low<daily_low2) & (daily_high2>daily_high3) & (daily_low2>daily_low3)']\n",
    "\n",
    "\n",
    "    pattern_dict['p86'] = ['daily_close2 = daily_close(dataset,2)',\n",
    "                           'daily_close3 = daily_close(dataset,3)',\n",
    "                           'dataset[\"p86\"] = (dataset.daily_close>daily_close2) & (daily_close2>daily_close3)']\n",
    "    \n",
    "    pattern_dict['p87'] = ['daily_close2 = daily_close(dataset,2)',\n",
    "                           'daily_close3 = daily_close(dataset,3)',\n",
    "                           'dataset[\"p87\"] = (dataset.daily_close<daily_close2) & (daily_close2<daily_close3)']\n",
    "    \n",
    "    pattern_dict['p88'] = ['daily_open2 = daily_open(dataset,2)',\n",
    "                           'daily_high2 = daily_high(dataset,2)',\n",
    "                           'daily_low2 = daily_low(dataset,2)',\n",
    "                           'daily_close2 = daily_close(dataset,2)',                           \n",
    "                           'myAvgPriceD1 = round(((dataset.daily_open + dataset.daily_high +dataset.daily_low + dataset.daily_close)/4),2)',\n",
    "                           'myAvgPriceD2 = round(((daily_open2 + daily_high2 + daily_low2 + daily_close2)/4),2)',\n",
    "                           'dataset[\"p88\"] = (myAvgPriceD1 > myAvgPriceD2)']\n",
    "    \n",
    "    pattern_dict['p89'] = ['daily_open2 = daily_open(dataset,2)',\n",
    "                           'daily_high2 = daily_high(dataset,2)',\n",
    "                           'daily_low2 = daily_low(dataset,2)',\n",
    "                           'daily_close2 = daily_close(dataset,3)', \n",
    "                           'daily_open3 = daily_open(dataset,3)',\n",
    "                           'daily_high3 = daily_high(dataset,3)',\n",
    "                           'daily_low3 = daily_low(dataset,3)',\n",
    "                           'daily_close3 = daily_close(dataset,3)', \n",
    "                           'myAvgPriceD1 = round(((dataset.daily_open + dataset.daily_high + dataset.daily_low + dataset.daily_close)/4),2)',\n",
    "                           'myAvgPriceD2 = round(((daily_open2 + daily_high2 + daily_low2 + daily_close2)/4),2)',\n",
    "                           'myAvgPriceD3 = round(((daily_open3 + daily_high3 + daily_low3 + daily_close3)/4),2)',\n",
    "                           'dataset[\"p89\"] = (myAvgPriceD1 > myAvgPriceD2) & (myAvgPriceD1 > myAvgPriceD3)']\n",
    "    \n",
    "    pattern_dict['p90'] = ['daily_open2 = daily_open(dataset,2)',\n",
    "                           'daily_high2 = daily_high(dataset,2)',\n",
    "                           'daily_low2 = daily_low(dataset,2)',\n",
    "                           'daily_close2 = daily_close(dataset,2)',                           \n",
    "                           'myAvgPriceD1 = round(((dataset.daily_open + dataset.daily_high + dataset.daily_low + dataset.daily_close)/4),2)',\n",
    "                           'myAvgPriceD2 = round(((daily_open2 + daily_high2 + daily_low2 + daily_close2)/4),2)',\n",
    "                           'dataset[\"p90\"] = (myAvgPriceD1 < myAvgPriceD2)']\n",
    "    \n",
    "    pattern_dict['p91'] = ['daily_open2 = daily_open(dataset,2)',\n",
    "                           'daily_high2 = daily_high(dataset,2)',\n",
    "                           'daily_low2 = daily_low(dataset,2)',\n",
    "                           'daily_close2 = daily_close(dataset,3)', \n",
    "                           'daily_open3 = daily_open(dataset,3)',\n",
    "                           'daily_high3 = daily_high(dataset,3)',\n",
    "                           'daily_low3 = daily_low(dataset,3)',\n",
    "                           'daily_close3 = daily_close(dataset,3)', \n",
    "                           'myAvgPriceD1 = round(((dataset.daily_open + dataset.daily_high + dataset.daily_low + dataset.daily_close)/4),2)',\n",
    "                           'myAvgPriceD2 = round(((daily_open2 + daily_high2 + daily_low2 + daily_close2)/4),2)',\n",
    "                           'myAvgPriceD3 = round(((daily_open3 + daily_high3 + daily_low3 + daily_close3)/4),2)',\n",
    "                           'dataset[\"p91\"] = (myAvgPriceD1 < myAvgPriceD2) & (myAvgPriceD1 < myAvgPriceD3)']\n",
    "\n",
    "    pattern_dict['p92'] = ['daily_high2 = daily_high(dataset,2)',\n",
    "                           'dataset[\"p92\"] = (dataset.daily_close>daily_high2)']\n",
    "    \n",
    "    pattern_dict['p93'] = ['daily_high2 = daily_high(dataset,2)',\n",
    "                           'dataset[\"p93\"] = (dataset.daily_close<daily_high2)']\n",
    "    \n",
    "    pattern_dict['p94'] = ['daily_high2 = daily_high(dataset,2)',\n",
    "                           'daily_high3 = daily_high(dataset,3)',\n",
    "                           'dataset[\"p94\"] = (dataset.daily_close>daily_high2) & (dataset.daily_close>daily_high3)']\n",
    "    \n",
    "    pattern_dict['p95'] = ['daily_low2 = daily_low(dataset,2)',\n",
    "                           'daily_low3 = daily_low(dataset,3)',\n",
    "                           'dataset[\"p95\"] = (dataset.daily_close<daily_low2) & (dataset.daily_close<daily_low3) ']\n",
    "    \n",
    "    pattern_dict['p96'] = ['dataset[\"p95\"] = True']\n",
    "    \n",
    "    pattern_dict['p97'] = ['daily_low2 = daily_low(dataset,2)',\n",
    "                           'daily_low3 = daily_low(dataset,3)',\n",
    "                           'daily_low4 = daily_low(dataset,4)',\n",
    "                           'dataset[\"p97\"] = (dataset.daily_close<daily_low2) & (dataset.daily_close<daily_low3) & (dataset.daily_close<daily_low4)']\n",
    "    \n",
    "    pattern_dict['p98'] = ['daily_high2 = daily_high(dataset,2)',\n",
    "                           'daily_open2 = daily_open(dataset,2)',\n",
    "                           'dataset[\"p98\"] = (dataset.daily_close>daily_high2) & (dataset.daily_close>dataset.daily_open) & (dataset.daily_close<daily_open2)']\n",
    "    \n",
    "    pattern_dict['p99'] = ['daily_low2 = daily_low(dataset,2)',\n",
    "                           'daily_open2 = daily_open(dataset,2)',\n",
    "                           'dataset[\"p99\"] = (dataset.daily_close<daily_low2) & (dataset.daily_close<dataset.daily_open) & (dataset.daily_close>daily_open2)']\n",
    "    \n",
    "    pattern_dict['p100'] = ['daily_low2 = daily_low(dataset,2)',\n",
    "                            'daily_high2 = daily_high(dataset,2)',\n",
    "                            'myRangeD1 = dataset.daily_high - dataset.daily_low',\n",
    "                            'myRangeD2 = daily_high2 - daily_low2',\n",
    "                            'dataset[\"p100\"] = (myRangeD1<myRangeD2)']\n",
    "    \n",
    "    pattern_dict['p101'] = ['daily_low5 = daily_low(dataset,5)',\n",
    "                            'daily_high5 = daily_high(dataset,5)',\n",
    "                            'myRangeD1 = dataset.daily_high - dataset.daily_low',\n",
    "                            'myRangeD5 = daily_high5 - daily_low5',\n",
    "                            'dataset[\"p101\"] = (myRangeD1<myRangeD5)']\n",
    "    \n",
    "    pattern_dict['p102'] = ['daily_high2 = daily_high(dataset,2)',\n",
    "                           'daily_high3 = daily_high(dataset,3)',\n",
    "                           'daily_high4 = daily_high(dataset,4)',\n",
    "                           'daily_high5 = daily_high(dataset,5)',\n",
    "                           'daily_high6 = daily_high(dataset,6)',\n",
    "                           'daily_high7 = daily_high(dataset,7)',\n",
    "                           'daily_high8 = daily_high(dataset,8)',\n",
    "                           'daily_high9 = daily_high(dataset,9)',\n",
    "                           'daily_high10 = daily_high(dataset,10)',\n",
    "                           'daily_low2 = daily_low(dataset,2)',\n",
    "                           'daily_low3 = daily_low(dataset,3)',\n",
    "                           'daily_low4 = daily_low(dataset,4)',\n",
    "                           'daily_low5 = daily_low(dataset,5)',\n",
    "                           'daily_low6 = daily_low(dataset,5)',\n",
    "                           'daily_low7 = daily_low(dataset,7)',\n",
    "                           'daily_low8 = daily_low(dataset,8)',\n",
    "                           'daily_low9 = daily_low(dataset,9)',\n",
    "                           'daily_low10 = daily_low(dataset,10)',  \n",
    "                           'myRangeMaxD10 = (highest_serie(dataset.daily_high,daily_high2,daily_high3,daily_high4,daily_high5,daily_high6,daily_high7,daily_high8,daily_high9,daily_high10) - (lowest_serie(dataset.daily_low,daily_low2,daily_low3,daily_low4,daily_low5,daily_low6,daily_low7,daily_low8,daily_low9,daily_low10)))',\n",
    "                           'myRangeD1 = dataset.daily_high - dataset.daily_low',                            \n",
    "                           'dataset[\"p102\"] = (myRangeD1<myRangeMaxD10)']\n",
    "    \n",
    "    pattern_dict['p103'] = ['daily_high2 = daily_high(dataset,2)',\n",
    "                           'daily_high3 = daily_high(dataset,3)',\n",
    "                           'daily_high4 = daily_high(dataset,4)',\n",
    "                           'daily_high5 = daily_high(dataset,5)',\n",
    "                           'daily_high6 = daily_high(dataset,6)',\n",
    "                           'daily_high7 = daily_high(dataset,7)',\n",
    "                           'daily_high8 = daily_high(dataset,8)',\n",
    "                           'daily_high9 = daily_high(dataset,9)',\n",
    "                           'daily_high0 = daily_high(dataset,10)',\n",
    "                           'daily_low2 = daily_low(dataset,2)',\n",
    "                           'daily_low3 = daily_low(dataset,3)',\n",
    "                           'daily_low4 = daily_low(dataset,4)',\n",
    "                           'daily_low5 = daily_low(dataset,5)',\n",
    "                           'daily_low6 = daily_low(dataset,5)',\n",
    "                           'daily_low7 = daily_low(dataset,7)',\n",
    "                           'daily_low8 = daily_low(dataset,8)',\n",
    "                           'daily_low9 = daily_low(dataset,9)',\n",
    "                           'daily_low0 = daily_low(dataset,10)', \n",
    "                           'myRangeDelay1MaxD10 = highest_serie(daily_high2,daily_high3,daily_high4,daily_high5,daily_high6,daily_high7,daily_high8,daily_high9,daily_high0) - \\\n",
    "                            lowest_serie(daily_low2,daily_low3,daily_low4,daily_low5,daily_low6,daily_low7,daily_low8,daily_low9,daily_low0)',\n",
    "                            'myRangeD1 = dataset.daily_high - dataset.daily_low',                            \n",
    "                            'dataset[\"p103\"] = (myRangeD1>myRangeDelay1MaxD10)']\n",
    "    \n",
    "\n",
    "    pattern_dict['p104'] = ['daily_high2 = daily_high(dataset,2)',\n",
    "                            'daily_high3 = daily_high(dataset,3)',\n",
    "                            'daily_high4 = daily_high(dataset,4)',\n",
    "                            'myMaxDelay1HighD4 =  highest_serie(daily_high2,daily_high3,daily_high4)',\n",
    "                            'dataset[\"p104\"] = (dataset.daily_close>myMaxDelay1HighD4)']\n",
    "    \n",
    "    pattern_dict['p105'] = ['daily_high2 = daily_high(dataset,2)',\n",
    "                            'daily_high3 = daily_high(dataset,3)',\n",
    "                            'daily_high4 = daily_high(dataset,4)',\n",
    "                            'daily_high5 = daily_high(dataset,5)',\n",
    "                            'myMaxDelay1HighD5 = highest_serie(daily_high2,daily_high3,daily_high4,daily_high5)',\n",
    "                            'dataset[\"p105\"] = (dataset.daily_close>myMaxDelay1HighD5)']\n",
    "    \n",
    "    pattern_dict['p106'] = ['daily_low2 = daily_low(dataset,2)',\n",
    "                           'daily_low3 = daily_low(dataset,3)',\n",
    "                           'daily_low4 = daily_low(dataset,4)',\n",
    "                            'myMinDelay1LowD4 = lowest_serie(daily_low2,daily_low3,daily_low4)',\n",
    "                            'dataset[\"p106\"] = (dataset.daily_close<myMinDelay1LowD4)']\n",
    "    \n",
    "    pattern_dict['p107'] = ['daily_low2 = daily_low(dataset,2)',\n",
    "                           'daily_low3 = daily_low(dataset,3)',\n",
    "                           'daily_low4 = daily_low(dataset,4)',\n",
    "                           'daily_low5 = daily_low(dataset,5)',                           \n",
    "                           'myMinDelay1LowD5 = lowest_serie(daily_low2,daily_low3,daily_low4,daily_low5)',\n",
    "                           'dataset[\"p107\"] = (dataset.daily_close<myMinDelay1LowD5)']\n",
    "    \n",
    "    pattern_dict['p108'] = ['dataset[\"p108\"] = True ']\n",
    "    pattern_dict['p109'] = ['dataset[\"p109\"] = True ']\n",
    "    pattern_dict['p110'] = ['dataset[\"p110\"] = True ']\n",
    "    pattern_dict['p111'] = ['dataset[\"p111\"] = True ']\n",
    "    pattern_dict['p112'] = ['dataset[\"p112\"] = True ']\n",
    "    pattern_dict['p113'] = ['dataset[\"p113\"] = True ']\n",
    "\n",
    "    \n",
    "    pattern_dict['p114'] = ['daily_close2 = daily_close(dataset,2)',\n",
    "                            'dataset[\"p114\"] = (dataset.daily_open<daily_close2)']\n",
    "    \n",
    "    pattern_dict['p115'] = ['daily_low3 = daily_low(dataset,3)',\n",
    "                            'dataset[\"p115\"] = (dataset.daily_low<daily_low3)']\n",
    "    \n",
    "    pattern_dict['p116'] = ['daily_low2 = daily_low(dataset,2)',\n",
    "                            'daily_high2 = daily_high(dataset,2)',\n",
    "                            'dataset[\"p116\"] = (dataset.daily_high-dataset.daily_low)<(daily_high2-daily_low2)']\n",
    "    \n",
    "    pattern_dict['p117'] = ['daily_high3 = daily_high(dataset,3)',\n",
    "                            'dataset[\"p117\"] = (dataset.daily_high>daily_high3)']\n",
    "    \n",
    "    pattern_dict['p118'] = ['daily_low2 = daily_low(dataset,2)',\n",
    "                            'dataset[\"p118\"] = (dataset.daily_low<daily_low2)']\n",
    "    \n",
    "    pattern_dict['p119'] = ['daily_high2 = daily_high(dataset,2)',\n",
    "                            'dataset[\"p119\"] = (dataset.daily_high>daily_high2)']\n",
    "    \n",
    "    pattern_dict['p120'] = ['daily_open5 = daily_open(dataset,5)',\n",
    "                            'daily_high2 = daily_high(dataset,2)',\n",
    "                            'daily_high3 = daily_high(dataset,3)',\n",
    "                            'daily_high4 = daily_high(dataset,4)',\n",
    "                            'daily_high5 = daily_high(dataset,5)',\n",
    "                            'daily_low2 = daily_low(dataset,2)',\n",
    "                            'daily_low3 = daily_low(dataset,3)',\n",
    "                            'daily_low4 = daily_low(dataset,4)',\n",
    "                            'daily_low5 = daily_low(dataset,5)',                            \n",
    "                            'myMaxHighD5 = highest_serie(dataset.daily_high,daily_high2,daily_high3,daily_high4,daily_high5)',\n",
    "                            'myMinLowD5 = lowest_serie(dataset.daily_low,daily_low2,daily_low3,daily_low4,daily_low5)',\n",
    "                            'dataset[\"p120\"] = (abs(daily_open5-dataset.daily_close)<0.66*(myMaxHighD5-myMinLowD5))']\n",
    "    \n",
    "    pattern_dict['p121'] = ['daily_high2 = daily_high(dataset,2)',\n",
    "                            'daily_high3 = daily_high(dataset,3)',\n",
    "                            'daily_high4 = daily_high(dataset,4)',\n",
    "                            'daily_high5 = daily_high(dataset,5)',\n",
    "                            'daily_low2 = daily_low(dataset,2)',\n",
    "                            'daily_low3 = daily_low(dataset,3)',\n",
    "                            'daily_low4 = daily_low(dataset,4)',\n",
    "                            'daily_low5 = daily_low(dataset,5)',                            \n",
    "                            'myMaxHighD5 = highest_serie(dataset.daily_high,daily_high2,daily_high3,daily_high4,daily_high5)',\n",
    "                            'myMinLowD5 = lowest_serie(dataset.daily_low,daily_low2,daily_low3,daily_low4,daily_low5)',\n",
    "                            'dataset[\"p121\"] = (abs(dataset.daily_open-dataset.daily_close)<0.66*(myMaxHighD5-myMinLowD5))']\n",
    "    \n",
    "    \n",
    "    pattern_dict['p122'] = ['dataset[\"p122\"] = (abs(dataset.daily_open-dataset.daily_close)<0.66*(dataset.daily_high-dataset.daily_low))']\n",
    "        \n",
    "    pattern_dict['p123'] = ['daily_low2 = daily_low(dataset,2)',\n",
    "                            'daily_high2 = daily_high(dataset,2)',\n",
    "                            'daily_high3 = daily_high(dataset,3)',\n",
    "                            'daily_low3 = daily_low(dataset,3)',\n",
    "                            'daily_high4 = daily_high(dataset,4)',\n",
    "                            'daily_low4 = daily_low(dataset,4)',                            \n",
    "                            'dataset[\"p123\"] = (dataset.daily_high-dataset.daily_low)>0.66*(((dataset.daily_high-dataset.daily_low)+\\\n",
    "                            (daily_high2-daily_low2)+(daily_high3-daily_low3)+\\\n",
    "                            (daily_high4-daily_low4)/4))']\n",
    "    \n",
    "    pattern_dict['p124'] = ['daily_low2 = daily_low(dataset,2)',\n",
    "                            'daily_high2 = daily_high(dataset,2)',\n",
    "                            'daily_high3 = daily_high(dataset,3)',\n",
    "                            'daily_low3 = daily_low(dataset,3)',\n",
    "                            'daily_high4 = daily_high(dataset,4)',\n",
    "                            'daily_low4 = daily_low(dataset,4)',                            \n",
    "                            'dataset[\"p124\"] = (dataset.daily_high-dataset.daily_low)<0.66*(((dataset.daily_high-dataset.daily_low)+\\\n",
    "                            (daily_high2-daily_low2)+(daily_high3-daily_low3)+\\\n",
    "                            (daily_high4-daily_low4)/4))']\n",
    "        \n",
    "    \n",
    "    pattern_dict['p125'] = ['weekly_low1 = weekly_low(dataset,1)',\n",
    "                            'weekly_open1 = weekly_open(dataset,1)',\n",
    "                            'weekly_close1 = weekly_close(dataset,1)',\n",
    "                            'weekly_high1 = weekly_high(dataset,1)',        \n",
    "                            'dataset[\"p125\"] = abs((weekly_open1-weekly_close1)<0.75*(weekly_high1-weekly_low1))']\n",
    "    \n",
    "    pattern_dict['p125'] = ['weekly_low1 = weekly_low(dataset,1)',\n",
    "                            'weekly_open1 = weekly_open(dataset,1)',\n",
    "                            'weekly_close1 = weekly_close(dataset,1)',\n",
    "                            'weekly_high1 = weekly_high(dataset,1)',        \n",
    "                            'dataset[\"p125\"] = abs((weekly_open1-weekly_close1)<0.75*(weekly_high1-weekly_low1))']\n",
    "        \n",
    "    pattern_dict['p126'] = ['weekly_low1 = weekly_low(dataset,1)',\n",
    "                            'weekly_open1 = weekly_open(dataset,1)',\n",
    "                            'weekly_close1 = weekly_close(dataset,1)',\n",
    "                            'weekly_high1 = weekly_high(dataset,1)',        \n",
    "                            'dataset[\"p126\"] = abs((weekly_open1-weekly_close1)<0.50*(weekly_high1-weekly_low1))']\n",
    "    \n",
    "    pattern_dict['p127'] = ['weekly_low1 = weekly_low(dataset,1)',\n",
    "                            'weekly_open1 = weekly_open(dataset,1)',\n",
    "                            'weekly_close1 = weekly_close(dataset,1)',\n",
    "                            'weekly_high1 = weekly_high(dataset,1)',        \n",
    "                            'dataset[\"p127\"] = abs((weekly_open1-weekly_close1)<0.25*(weekly_high1-weekly_low1))']\n",
    "    \n",
    "    pattern_dict['p128'] = ['daily_high2 = daily_high(dataset,2)',\n",
    "                            'daily_high3 = daily_high(dataset,3)',\n",
    "                            'daily_high4 = daily_high(dataset,4)',\n",
    "                            'daily_low2 = daily_low(dataset,2)',\n",
    "                            'daily_low3 = daily_low(dataset,3)',\n",
    "                            'daily_low4 = daily_low(dataset,4)',       \n",
    "                            'dataset[\"p128\"] = ((daily_high4-daily_low4)<(daily_high3-daily_low3)) & ((daily_high3-daily_low3)<(daily_high2-daily_low2)) & ((daily_high2-daily_low2)<(dataset.daily_high-dataset.daily_low))']\n",
    "    \n",
    "    pattern_dict['p129'] = ['daily_high2 = daily_high(dataset,2)',\n",
    "                            'daily_high3 = daily_high(dataset,3)',\n",
    "                            'daily_high4 = daily_high(dataset,4)',\n",
    "                            'daily_low2 = daily_low(dataset,2)',\n",
    "                            'daily_low3 = daily_low(dataset,3)',\n",
    "                            'daily_low4 = daily_low(dataset,4)',       \n",
    "                            'dataset[\"p129\"] = ((daily_high3-daily_low3)<(daily_high2-daily_low2))&((daily_high2-daily_low2)<(dataset.daily_high-dataset.daily_low))']\n",
    "    \n",
    "    pattern_dict['p130'] = ['daily_high2 = daily_high(dataset,2)',\n",
    "                            'daily_high3 = daily_high(dataset,3)',\n",
    "                            'daily_high4 = daily_high(dataset,4)',\n",
    "                            'daily_low2 = daily_low(dataset,2)',\n",
    "                            'daily_low3 = daily_low(dataset,3)',\n",
    "                            'daily_low4 = daily_low(dataset,4)',       \n",
    "                            'dataset[\"p130\"] = ((daily_high4-daily_low4)>(daily_high3-daily_low3)) & ((daily_high3-daily_low3)>(daily_high2-daily_low2)) & ((daily_high2-daily_low2)>(dataset.daily_high-dataset.daily_low))']\n",
    "    \n",
    "    pattern_dict['p131'] = ['daily_high2 = daily_high(dataset,2)',\n",
    "                            'daily_high3 = daily_high(dataset,3)',\n",
    "                            'daily_high4 = daily_high(dataset,4)',\n",
    "                            'daily_low2 = daily_low(dataset,2)',\n",
    "                            'daily_low3 = daily_low(dataset,3)',\n",
    "                            'daily_low4 = daily_low(dataset,4)',       \n",
    "                            'dataset[\"p131\"] = ((daily_high3-daily_low3)>(daily_high2-daily_low2)) & ((daily_high2-daily_low2)>(dataset.daily_high-dataset.daily_low))']\n",
    "    \n",
    "    pattern_dict['p132'] = ['dataset[\"p132\"] = (dataset.daily_close-dataset.daily_low)<(dataset.daily_high-dataset.daily_low)/4']\n",
    "    \n",
    "    pattern_dict['p133'] = ['dataset[\"p133\"] = (dataset.daily_high-dataset.daily_close)<(dataset.daily_high-dataset.daily_low)/4']\n",
    "    \n",
    "    pattern_dict['p134'] = ['dataset[\"p134\"] = (dataset.daily_close-dataset.daily_low)<(dataset.daily_high-dataset.daily_low)/3']\n",
    "    \n",
    "    pattern_dict['p135'] = ['dataset[\"p135\"] = (dataset.daily_high-dataset.daily_close)<(dataset.daily_high-dataset.daily_low)/4']\n",
    "    \n",
    "    pattern_dict['p136'] = ['dataset[\"p136\"] = (dataset.daily_close-dataset.daily_low)<(dataset.daily_high-dataset.daily_low)/2']\n",
    "    \n",
    "    pattern_dict['p137'] = ['dataset[\"p137\"] = (dataset.daily_high-dataset.daily_close)<(dataset.daily_high-dataset.daily_low)/2']\n",
    "    \n",
    "    pattern_dict['p138'] = ['daily_open0 = daily_open(dataset,0)',                            \n",
    "                            'dataset[\"p138\"] = (daily_open0 > dataset.daily_high)']\n",
    "    \n",
    "    pattern_dict['p139'] = ['daily_open0 = daily_open(dataset,0)',                            \n",
    "                            'dataset[\"p139\"] = (daily_open0<dataset.daily_high)']\n",
    "    \n",
    "    pattern_dict['p140'] = ['daily_open0 = daily_open(dataset,0)',                            \n",
    "                            'dataset[\"p140\"] = (daily_open0<dataset.daily_low)']\n",
    "    \n",
    "    pattern_dict['p141'] = ['daily_open0 = daily_open(dataset,0)',\n",
    "                            'daily_high2 = daily_high(dataset,2)',                            \n",
    "                            'dataset[\"p141\"] = (daily_open0<=daily_high2)']\n",
    "    \n",
    "    pattern_dict['p142'] = ['daily_open0 = daily_open(dataset,0)',\n",
    "                            'daily_high2 = daily_high(dataset,2)',                            \n",
    "                            'dataset[\"p142\"] = (daily_open0>daily_high2)']\n",
    "    \n",
    "    pattern_dict['p143'] = ['daily_open0 = daily_open(dataset,0)',\n",
    "                            'daily_low3 = daily_low(dataset,3)',                            \n",
    "                            'dataset[\"p143\"] = (daily_open0>daily_low3)']\n",
    "    \n",
    "    pattern_dict['p144'] = ['daily_open0 = daily_open(dataset,0)',\n",
    "                            'daily_open2 = daily_open(dataset,2)',                            \n",
    "                            'dataset[\"p144\"] = (daily_open0>daily_open2)']\n",
    "    \n",
    "    pattern_dict['p145'] = ['daily_open0 = daily_open(dataset,0)',\n",
    "                            'daily_open2 = daily_open(dataset,2)',                            \n",
    "                            'dataset[\"p145\"] = (daily_open0<daily_open2)']\n",
    "    \n",
    "    pattern_dict['p146'] = ['daily_low2 = daily_low(dataset,2)',                            \n",
    "                            'dataset[\"p146\"] = (dataset.daily_close>daily_low2)']\n",
    "    \n",
    "    pattern_dict['p147'] = ['daily_low2 = daily_low(dataset,2)',                            \n",
    "                            'dataset[\"p147\"] = (dataset.daily_close<daily_low2)']\n",
    "    \n",
    "    pattern_dict['p148'] = ['daily_close2 = daily_close(dataset,2)',                            \n",
    "                            'dataset[\"p148\"] = (dataset.daily_close>daily_close2)']\n",
    "    \n",
    "    pattern_dict['p149'] = ['daily_close2 = daily_close(dataset,2)',                            \n",
    "                            'dataset[\"p149\"] = (dataset.daily_close<daily_close2)']\n",
    "    \n",
    "    pattern_dict['p150'] = ['daily_close2 = daily_close(dataset,2)',\n",
    "                            'daily_close3 = daily_close(dataset,3)',                            \n",
    "                            'dataset[\"p150\"] = (daily_close2 < daily_close3)']\n",
    "\n",
    "    return pattern_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cde372-d37d-4ccd-92fa-58177e75b64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a2aa08-4865-4d64-8bc1-5fb9a2390eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carica_storico_second(file_name,second_history,uct_offset,type_session,resample_tf,IS,OOS,custom_session_start,custom_session_stop,noise,pct_noise):\n",
    "    import os\n",
    "    import talib as ta\n",
    "    \n",
    "    os.chdir(dir_history)\n",
    "    data = load_data_intraday_fast(file_name)  \n",
    "    data_s = load_data_intraday_fast(second_history) \n",
    "    \n",
    "    data=data.resample(\"5min\").mean()\n",
    "    data_s=data_s.resample(\"5min\").mean()\n",
    "    \n",
    "    \n",
    "    data=pd.concat([data,data_s],axis=1).fillna(method=\"ffill\")\n",
    "    data=data.dropna()\n",
    "    data=data.loc[IS:]\n",
    "    data_s=data.iloc[:,5:]\n",
    "    data=data.iloc[:,:5]\n",
    "    \n",
    "    display(data.tail(1),data_s.tail(1))\n",
    "    print(len(data),len(data_s))\n",
    "    \n",
    "    print(\"Caricato storico\")\n",
    "    data = data.sort_index(ascending=True)\n",
    "    data.index = data.index.tz_localize('Etc/Zulu')\n",
    "    data.index = data.index.tz_convert(uct_offset )\n",
    "    data.index = data.index.tz_localize(None)\n",
    "    data = data[data!=0]\n",
    "    data = data.iloc[1:-1]\n",
    "    \n",
    "    data_s = data_s.sort_index(ascending=True)\n",
    "    data_s.index = data_s.index.tz_localize('Etc/Zulu')\n",
    "    data_s.index = data_s.index.tz_convert(uct_offset )\n",
    "    data_s.index = data_s.index.tz_localize(None)\n",
    "    data_s = data_s[data_s!=0]\n",
    "    data_s = data_s.iloc[1:-1]\n",
    "    \n",
    "    print(\"Resample dei dati\")\n",
    "    if type_session == 1:\n",
    "        dataset=resample_custom_session(\"5min\",resample_tf ,data,custom_session_start,custom_session_stop)\n",
    "        dataset_s=resample_custom_session(\"5min\",resample_tf ,data_s,custom_session_start,custom_session_stop)\n",
    "    if type_session == 0:    \n",
    "        dataset=resample_standard_session(\"5min\",resample_tf,data)\n",
    "        dataset_s=resample_standard_session(\"5min\",resample_tf,data_s)\n",
    "        \n",
    "    dataset=pd.concat([dataset,dataset_s],axis=1).fillna(method=\"ffill\")\n",
    "    dataset_s=dataset.iloc[:,5:]\n",
    "    dataset=dataset.iloc[:,:5]\n",
    "        \n",
    "    if noise == 1:\n",
    "        print(\"AGGIUNGO RUMORE\")\n",
    "        dataset=add_noise(dataset.open, dataset.high, dataset.low, dataset.close,dataset.volume, pct_noise)\n",
    "\n",
    "    dataset = dataset.loc[IS:OOS]\n",
    "    dataset_s = dataset_s.loc[IS:OOS]\n",
    "    dataset[\"dayofweek\"] = dataset.index.dayofweek\n",
    "    dataset[\"day\"] = dataset.index.day\n",
    "    dataset[\"month\"] = dataset.index.month\n",
    "    dataset[\"year\"] = dataset.index.year\n",
    "    dataset[\"dayofyear\"] = dataset.index.dayofyear\n",
    "    dataset[\"quarter\"] = dataset.index.quarter\n",
    "    dataset[\"hour\"] = dataset.index.hour\n",
    "    dataset[\"minute\"] = dataset.index.minute\n",
    "    dataset[\"daily_open\"] = daily_open(dataset,1)\n",
    "    dataset[\"daily_high\"] = daily_high(dataset,1)\n",
    "    dataset[\"daily_low\"] = daily_low(dataset,1)\n",
    "    dataset[\"daily_close\"] = daily_close(dataset,1)\n",
    "    dataset[\"AVGPRICE\"]=ta.AVGPRICE(dataset.open, dataset.high, dataset.low, dataset.close)\n",
    "    dataset[\"MEDPRICE\"]=ta.MEDPRICE(dataset.high, dataset.low)\n",
    "    dataset[\"TYPPRICE\"]=ta.TYPPRICE(dataset.high, dataset.low, dataset.close)\n",
    "    dataset[\"WCLPRICE\"]=ta.WCLPRICE(dataset.high, dataset.low, dataset.close)\n",
    "    history=dataset.copy()\n",
    "    \n",
    "    print(\"Carico indicatori\")\n",
    "    import talib as ta\n",
    "    applica_indicatori=apply_indicator(dataset_s)\n",
    "    dataset=applica_indicatori[0]\n",
    "    dataset_ind = dataset.iloc[:,5:].copy()\n",
    "\n",
    "\n",
    "    rules=dataset.iloc[:,5:].T.values\n",
    "    rule_formulas=np.arange(len(rules))\n",
    "    \n",
    "    history.tail(10)\n",
    "    \n",
    "    history.close.plot(figsize=(20,10),title=SIMBOLO+\"_\"+RESAMPLE_TF)\n",
    "    print(\"Fatto\")\n",
    "    return dataset , dataset_ind , history , rules, rule_formulas\n",
    "\n",
    "def target(direzione,_search,dataset):\n",
    "    _search=_search\n",
    "    close=history.close.copy()\n",
    "    t_m=[]\n",
    "    if direzione == \"long\":\n",
    "        t_m=close-close.shift(_search)\n",
    "        t_m=t_m.shift(-_search).fillna(0)\n",
    "        t_m=np.array(t_m * bigpointvalue)\n",
    "    else:\n",
    "        t_m=close-close.shift(_search)\n",
    "        t_m=t_m.shift(-_search).fillna(0)\n",
    "        t_m=t_m*(-1)\n",
    "        t_m=np.array(t_m * bigpointvalue)\n",
    "    return t_m\n",
    "\n",
    "def environment_second(simbolo,simbolo2,type_session,resample_tf,start_date,end_date):\n",
    "               \n",
    "    file_name = check_history_name(simbolo,dir_history)\n",
    "    file_name2 = check_history_name(simbolo,dir_history) #simbolo2+\".csv\"\n",
    "\n",
    "    IS = start_date\n",
    "    OOS = end_date\n",
    "\n",
    "    bigpointvalue=valori[valori.Physical==simbolo].BigPointValue.values[0]\n",
    "    tick=valori[valori.Physical==simbolo].Tick.values[0]\n",
    "    utc_offset = valori[valori.Physical==simbolo].Exchange_Time.values[0]\n",
    "    custom_session_start = \"0\"+str(valori[valori.Physical==simbolo][\"Custom Open\"].values[0])\n",
    "    custom_session_stop = str(valori[valori.Physical==simbolo][\"Custom Close\"].values[0])\n",
    "    costi=valori[valori.Physical==simbolo].Cost.values[0]\n",
    "\n",
    "    dataset , dataset_ind , history , rules, rule_formulas = carica_storico_second(file_name,file_name2,utc_offset,type_session,resample_tf,IS,OOS,custom_session_start,custom_session_stop,0,0)\n",
    "    \n",
    "    return dataset , dataset_ind , history , rules, rule_formulas , bigpointvalue , tick , utc_offset , custom_session_start , custom_session_stop , costi \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9338a36f-ddc8-453c-a467-d83560c86b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carica_storico_second(file_name,second_history,uct_offset,type_session,resample_tf,IS,OOS,custom_session_start,custom_session_stop,noise,pct_noise):\n",
    "    import os\n",
    "    import talib as ta\n",
    "    \n",
    "    os.chdir(dir_history)\n",
    "    data = load_data_intraday_fast(file_name)  \n",
    "    data_s = load_data_intraday_fast(second_history) \n",
    "    \n",
    "    data=data.resample(\"5min\").mean()\n",
    "    data_s=data_s.resample(\"5min\").mean()\n",
    "    \n",
    "    \n",
    "    data=pd.concat([data,data_s],axis=1).fillna(method=\"ffill\")\n",
    "    data=data.dropna()\n",
    "    data=data.loc[IS:]\n",
    "    data_s=data.iloc[:,5:]\n",
    "    data=data.iloc[:,:5]\n",
    "    \n",
    "    display(data.tail(1),data_s.tail(1))\n",
    "    print(len(data),len(data_s))\n",
    "    \n",
    "    print(\"Caricato storico\")\n",
    "    data = data.sort_index(ascending=True)\n",
    "    data.index = data.index.tz_localize('Etc/Zulu')\n",
    "    data.index = data.index.tz_convert(uct_offset )\n",
    "    data.index = data.index.tz_localize(None)\n",
    "    data = data[data!=0]\n",
    "    data = data.iloc[1:-1]\n",
    "    \n",
    "    data_s = data_s.sort_index(ascending=True)\n",
    "    data_s.index = data_s.index.tz_localize('Etc/Zulu')\n",
    "    data_s.index = data_s.index.tz_convert(uct_offset )\n",
    "    data_s.index = data_s.index.tz_localize(None)\n",
    "    data_s = data_s[data_s!=0]\n",
    "    data_s = data_s.iloc[1:-1]\n",
    "    \n",
    "    print(\"Resample dei dati\")\n",
    "    if type_session == 1:\n",
    "        dataset=resample_custom_session(\"5min\",resample_tf ,data,custom_session_start,custom_session_stop)\n",
    "        dataset_s=resample_custom_session(\"5min\",resample_tf ,data_s,custom_session_start,custom_session_stop)\n",
    "    if type_session == 0:    \n",
    "        dataset=resample_standard_session(\"5min\",resample_tf,data)\n",
    "        dataset_s=resample_standard_session(\"5min\",resample_tf,data_s)\n",
    "        \n",
    "    dataset=pd.concat([dataset,dataset_s],axis=1).fillna(method=\"ffill\")\n",
    "    dataset_s=dataset.iloc[:,5:]\n",
    "    dataset=dataset.iloc[:,:5]\n",
    "        \n",
    "    if noise == 1:\n",
    "        print(\"AGGIUNGO RUMORE\")\n",
    "        dataset=add_noise(dataset.open, dataset.high, dataset.low, dataset.close,dataset.volume, pct_noise)\n",
    "\n",
    "    dataset = dataset.loc[IS:OOS]\n",
    "    dataset_s = dataset_s.loc[IS:OOS]\n",
    "    dataset[\"dayofweek\"] = dataset.index.dayofweek\n",
    "    dataset[\"day\"] = dataset.index.day\n",
    "    dataset[\"month\"] = dataset.index.month\n",
    "    dataset[\"year\"] = dataset.index.year\n",
    "    dataset[\"dayofyear\"] = dataset.index.dayofyear\n",
    "    dataset[\"quarter\"] = dataset.index.quarter\n",
    "    dataset[\"hour\"] = dataset.index.hour\n",
    "    dataset[\"minute\"] = dataset.index.minute\n",
    "    dataset[\"daily_open\"] = daily_open(dataset,1)\n",
    "    dataset[\"daily_high\"] = daily_high(dataset,1)\n",
    "    dataset[\"daily_low\"] = daily_low(dataset,1)\n",
    "    dataset[\"daily_close\"] = daily_close(dataset,1)\n",
    "    dataset[\"AVGPRICE\"]=ta.AVGPRICE(dataset.open, dataset.high, dataset.low, dataset.close)\n",
    "    dataset[\"MEDPRICE\"]=ta.MEDPRICE(dataset.high, dataset.low)\n",
    "    dataset[\"TYPPRICE\"]=ta.TYPPRICE(dataset.high, dataset.low, dataset.close)\n",
    "    dataset[\"WCLPRICE\"]=ta.WCLPRICE(dataset.high, dataset.low, dataset.close)\n",
    "    history=dataset.copy()\n",
    "    \n",
    "    print(\"Carico indicatori\")\n",
    "    import talib as ta\n",
    "    applica_indicatori=apply_indicator(dataset_s)\n",
    "    dataset=applica_indicatori[0]\n",
    "    dataset_ind = dataset.iloc[:,5:].copy()\n",
    "\n",
    "\n",
    "    rules=dataset.iloc[:,5:].T.values\n",
    "    rule_formulas=np.arange(len(rules))\n",
    "    \n",
    "    history.tail(10)\n",
    "    \n",
    "    history.close.plot(figsize=(20,10),title=SIMBOLO+\"_\"+RESAMPLE_TF)\n",
    "    print(\"Fatto\")\n",
    "    return dataset , dataset_ind , history , rules, rule_formulas\n",
    "\n",
    "def target(direzione,_search,dataset):\n",
    "    _search=_search\n",
    "    close=history.close.copy()\n",
    "    t_m=[]\n",
    "    if direzione == \"long\":\n",
    "        t_m=close-close.shift(_search)\n",
    "        t_m=t_m.shift(-_search).fillna(0)\n",
    "        t_m=np.array(t_m * bigpointvalue)\n",
    "    else:\n",
    "        t_m=close-close.shift(_search)\n",
    "        t_m=t_m.shift(-_search).fillna(0)\n",
    "        t_m=t_m*(-1)\n",
    "        t_m=np.array(t_m * bigpointvalue)\n",
    "    return t_m\n",
    "\n",
    "def environment_second(simbolo,simbolo2,type_session,resample_tf,start_date,end_date):\n",
    "               \n",
    "    file_name = check_history_name(simbolo,dir_history)\n",
    "    file_name2 = check_history_name(simbolo,dir_history) #simbolo2+\".csv\"\n",
    "\n",
    "    IS = start_date\n",
    "    OOS = end_date\n",
    "\n",
    "    bigpointvalue=valori[valori.Physical==simbolo].BigPointValue.values[0]\n",
    "    tick=valori[valori.Physical==simbolo].Tick.values[0]\n",
    "    utc_offset = valori[valori.Physical==simbolo].Exchange_Time.values[0]\n",
    "    custom_session_start = \"0\"+str(valori[valori.Physical==simbolo][\"Custom Open\"].values[0])\n",
    "    custom_session_stop = str(valori[valori.Physical==simbolo][\"Custom Close\"].values[0])\n",
    "    costi=valori[valori.Physical==simbolo].Cost.values[0]\n",
    "\n",
    "    dataset , dataset_ind , history , rules, rule_formulas = carica_storico_second(file_name,file_name2,utc_offset,type_session,resample_tf,IS,OOS,custom_session_start,custom_session_stop,0,0)\n",
    "    \n",
    "    return dataset , dataset_ind , history , rules, rule_formulas , bigpointvalue , tick , utc_offset , custom_session_start , custom_session_stop , costi \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
